<!doctype html><html lang=en-us class=theme-auto><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Lab01: Adding Vector - junuxyz</title><meta name=description content="Personal blog and thoughts"><meta name=author content="Junu"><link rel=stylesheet href=/css/main.css><link rel=icon type=image/x-icon href=/favicon.ico><script>const theme=localStorage.getItem("theme")||"auto";(theme==="dark"||theme==="auto"&&window.matchMedia("(prefers-color-scheme: dark)").matches)&&document.documentElement.classList.add("dark")</script></head><body><header class=header><div class=header-content><div class=logo><a href=https://junuxyz.github.io/>junuxyz</a></div><div class=header-right><nav class=nav><a href=/categories/thoughts/ class=nav-item>Thoughts</a>
<a href=/categories/ml/ class=nav-item>ML</a>
<a href=/posts/ class=nav-item>All Posts</a>
<a href=/about/ class=nav-item>About</a>
<a href=/tags/ class=nav-item>Tags</a></nav><button id=theme-toggle type=button aria-label="Toggle theme">
<svg class="sun-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
<svg class="moon-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></button></div></div></header><main class=main><div class=container><article class=post><header class=post-header><h1 class=post-title>Lab01: Adding Vector</h1><div class=post-meta><time datetime=2025-07-20T16:26:31+09:00>July 20, 2025</time><div class=post-tags><a href=/tags/labs/ class=tag>#labs</a>
<a href=/tags/pytorch/ class=tag>#pytorch</a>
<a href=/tags/triton/ class=tag>#triton</a></div></div></header><div class=post-content><p>This is a simple experiment to just get a feel of the abstraction PyTorch provides, and all the internal complexity hidden below. We will also compare the performance of basic vector addition between PyTorch, Triton, and CUDA.</p><p><em>Note: This experiment was done in NVIDIA RTX 3050ti laptop GPU</em></p><h3 id=vector-addition-in-pytorch>Vector Addition in PyTorch</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>size <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(size, device<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cuda&#39;</span>)
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(size, device<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cuda&#39;</span>)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>empty_like(a)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> a <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;PyTorch output:&#34;</span>)
</span></span><span style=display:flex><span>print(output)
</span></span></code></pre></div><ul><li><code>empty_like(a)</code> creates the same size, dtype, and device(&lsquo;cuda&rsquo;) as the input tensor <code>a</code>. It does not initialize the memory into something else, but use the garbage value of it so it&rsquo;s a bit faster than using <code>torch.zeros()</code> or <code>torch.ones()</code>.
The exact operation of vector addition is hidden in operator <code>+</code> in PyTorch.</li></ul><h3 id=vector-addition-in-triton>Vector Addition in Triton</h3><p><a href=https://github.com/triton-lang/triton>Triton</a> is an open source library ran by OpenAI, which aims to be easier to code than CUDA (fewer knobs to control, don&rsquo;t need to know as deep as CUDA) but doesn&rsquo;t lose the performance.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> triton
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> triton.language <span style=color:#66d9ef>as</span> tl
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@triton.jit</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_kernel</span>(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl<span style=color:#f92672>.</span>constexpr):
</span></span><span style=display:flex><span>	pid <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>program_id(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>	block_start <span style=color:#f92672>=</span> pid <span style=color:#f92672>*</span> BLOCK_SIZE
</span></span><span style=display:flex><span>	offsets <span style=color:#f92672>=</span> block_start <span style=color:#f92672>+</span> tl<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>0</span>, BLOCK_SIZE)
</span></span><span style=display:flex><span>	mask <span style=color:#f92672>=</span> offsets <span style=color:#f92672>&lt;</span> n_elements
</span></span><span style=display:flex><span>	x <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>load(x_ptr <span style=color:#f92672>+</span> offsets, mask<span style=color:#f92672>=</span>mask)
</span></span><span style=display:flex><span>	y <span style=color:#f92672>=</span> tl<span style=color:#f92672>.</span>load(y_ptr <span style=color:#f92672>+</span> offsets, mask<span style=color:#f92672>=</span>mask)
</span></span><span style=display:flex><span>	output <span style=color:#f92672>=</span> x <span style=color:#f92672>+</span> y
</span></span><span style=display:flex><span>	tl<span style=color:#f92672>.</span>store(output_ptr <span style=color:#f92672>+</span> offsets, output, mask<span style=color:#f92672>=</span>mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>triton_add</span>(a: torch<span style=color:#f92672>.</span>Tensor, b:torch<span style=color:#f92672>.</span>Tensor) <span style=color:#f92672>-&gt;</span> torch<span style=color:#f92672>.</span>Tensor:
</span></span><span style=display:flex><span>	output <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>empty_like(a)
</span></span><span style=display:flex><span>	n_elements <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>numel()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	BLOCK_SIZE <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>	grid <span style=color:#f92672>=</span> (triton<span style=color:#f92672>.</span>cdiv(n_elements, BLOCK_SIZE),)
</span></span><span style=display:flex><span>	add_kernel[grid](a, b, output, n_elements, BLOCK_SIZE<span style=color:#f92672>=</span>BLOCK_SIZE)
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> output
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>size <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(size, device<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cuda&#39;</span>)
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(size, device<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cuda&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> triton_add(a, b)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Triton output&#34;</span>)
</span></span><span style=display:flex><span>print(output)
</span></span></code></pre></div><p>In order to understand this, we need to understand memories and parallel programming. Since this lab is just a tiny experiment, we will not go deep into all the concepts but rather explain what is happening here.</p><p><code>triton_add</code></p><ul><li><code>n_elements = output.numel()</code> calculates the total work to do based on the output size which is 128 in this case.</li><li>We set <code>BLOCK_SIZE</code>, which defines the size of the data to process.</li><li><code>grid</code> calculates how much pids are needed to execute the elements according to BLOCK_SIZE. (In this case it&rsquo;s 128/128 so we only need a single program to execute vector addition) CPU then sends the instruction to GPU to proceed vector addition.<ul><li>We will talk about why the block size is set to 128 specifically, later in some other post (!TODO: I will link that part to here!)</li><li>cdiv is used to include the leftovers after dividing into BLOCK_SIZE. (eg. 130 dimentions / BLOCK_SIZE will result in 2 instead of 1)</li></ul></li></ul><p><code>add_kernel</code></p><ul><li><strong><code>@triton.jit</code></strong> is a special decorator that makes the function into machine language(or kernel) that can be run in GPU. This means unlike <code>triton_add</code> function, in <code>add_kernel</code>, we are using GPU programming. jit is short for just-in-time, which means the code compiles just in time as it runs.</li><li>As you can see in the parameter, we use pointers for inputs and output (<code>x_ptr</code>, <code>y_ptr</code>, and <code>output_ptr</code>). This is because we load data from the GPU RAM.</li><li><code>pid</code> or program ids are unique ids(eg. 0, 1, 2&mldr;) given to the GPU. Each program(kernel) checks its id.</li><li>each program uses its id(eg. 1) and calculate it with the amount of work it should proceed defined by BLOCK_SIZE. (eg. pid=1 should start from <code>1*128</code> to <code>2*128-1</code>)</li><li>Each programs parallely proceeds the process(in this example, it would be vector addition).</li><li>mask helps to check if the offsets do not exceed the actual data range.</li><li>Now, we take inputs from their pointers and <strong>load</strong> data based on the offests and mask.</li><li>The vector addition happens in the ALU in GPU.</li><li>Then we save the result to the output_ptr in GPU RAM.</li></ul><p>first tried to naively check the performance with shell&rsquo;s <code>time</code> command but figured out it was an inappropriate tool to check the actual performance between two codes.</p><p><code>time</code> command measures multiple things in the environment such as Python interpreter starting time, loading libraries, CUDA context initializing (which takes a lot longer than the actual vector addition), and the GPU operation.</p><p>The more accurate way to check performance is to measure using <code>torch.cuda.Event</code></p><p>I made a simple benchmark.py to measure the difference:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> add_triton <span style=color:#f92672>import</span> triton_add
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>benchmark_pytorch</span>(a,b):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> a <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_benchmark</span>(fn, <span style=color:#f92672>*</span>args):
</span></span><span style=display:flex><span>    start_event <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>Event(enable_timing<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    end_event <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>Event(enable_timing<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Warm-up for GPU</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>        fn(<span style=color:#f92672>*</span>args)
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>synchronize()
</span></span><span style=display:flex><span>    start_event<span style=color:#f92672>.</span>record()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>        fn(<span style=color:#f92672>*</span>args)
</span></span><span style=display:flex><span>    end_event<span style=color:#f92672>.</span>record()
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>synchronize()
</span></span><span style=display:flex><span>    elapsed_time_ms <span style=color:#f92672>=</span> start_event<span style=color:#f92672>.</span>elapsed_time(end_event)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> elapsed_time_ms <span style=color:#f92672>/</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>device <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;cuda&#39;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#39;cpu&#39;</span>
</span></span><span style=display:flex><span>size <span style=color:#f92672>=</span> <span style=color:#ae81ff>1024</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(size, device<span style=color:#f92672>=</span>device)
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(size, device<span style=color:#f92672>=</span>device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pytorch_time <span style=color:#f92672>=</span> run_benchmark(benchmark_pytorch, a, b)
</span></span><span style=display:flex><span>triton_time <span style=color:#f92672>=</span> run_benchmark(triton_add, a, b)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Vector size: </span><span style=color:#e6db74>{</span>size<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;PyTorch average time: </span><span style=color:#e6db74>{</span>pytorch_time<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> ms&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Triton average time: </span><span style=color:#e6db74>{</span>triton_time<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> ms&#34;</span>)
</span></span></code></pre></div><p><code>torch.cuda.synchronize()</code> is used to force CPU to <strong>wait</strong> until GPU operation is done. Since the default behavior between CPU and GPU are asynchronous, we use this command to check the precise amount of time of the GPU operation.</p><p>I&rsquo;ve tried with size = 128 but it was so short the noise took too much portion so I increased the size into <code>1024 * 1024</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>❯ python benchmark.py
</span></span><span style=display:flex><span>Vector size: <span style=color:#ae81ff>1048576</span>
</span></span><span style=display:flex><span>PyTorch average time: 0.124672 ms
</span></span><span style=display:flex><span>Triton average time: 0.124037 ms
</span></span></code></pre></div><p>result was almost the same.
We can conclude that vector addition is so simple + PyTorch optimized it well that there seems no room for optimizing vector addition better than PyTorch. PyTorch is as good.</p><p>I guess we will have to cover things computationally heavier, such as matmul.</p></div><nav class=post-nav><div class=nav-prev><a href=https://junuxyz.github.io/posts/a-shallow-dive-into-vscode-debugger/ class=nav-link><span class=nav-label>← Previous</span>
<span class=nav-title>A (shallow) Dive into VSCode Debugger</span></a></div></nav></article></div></main><footer class=footer><div class=footer-content><p>&copy; 2025 junuxyz. Built with <a href=https://gohugo.io/>Hugo</a>.</p></div></footer><script src=/js/theme.js></script></body></html>