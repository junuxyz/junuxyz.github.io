<!doctype html><html lang=en-us class=theme-auto><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Lab01: Adding Vector - junuxyz</title><meta name=description content="Personal blog and thoughts"><meta name=author content="Junu"><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/css/syntax.css><link rel=icon type=image/x-icon href=/favicon.ico><script>const theme=localStorage.getItem("theme")||"auto";(theme==="dark"||theme==="auto"&&window.matchMedia("(prefers-color-scheme: dark)").matches)&&document.documentElement.classList.add("dark")</script></head><body><header class=header><div class=header-content><div class=logo><a href=https://junuxyz.github.io/>junuxyz</a></div><div class=header-right><nav class=nav><a href=/categories/thoughts/ class=nav-item>Thoughts</a>
<a href=/categories/ml/ class=nav-item>ML</a>
<a href=/posts/ class=nav-item>All Posts</a>
<a href=/about/ class=nav-item>About</a>
<a href=/tags/ class=nav-item>Tags</a></nav><button id=theme-toggle type=button aria-label="Toggle theme">
<svg class="sun-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
<svg class="moon-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></button></div></div></header><main class=main><div class=container><article class=post><header class=post-header><h1 class=post-title>Lab01: Adding Vector</h1><div class=post-meta><time datetime=2025-07-20T16:26:31+09:00>July 20, 2025</time><div class=post-tags><a href=/tags/labs/ class=tag>#labs</a>
<a href=/tags/pytorch/ class=tag>#pytorch</a>
<a href=/tags/triton/ class=tag>#triton</a></div></div></header><div class=post-content><p>This is a simple experiment to just get a feel of the abstraction PyTorch provides, and all the internal complexity hidden below. We will also compare the performance of basic vector addition between PyTorch, Triton, and CUDA.</p><p><em>Note: This experiment was done in NVIDIA RTX 3050ti laptop GPU</em></p><h3 id=vector-addition-in-pytorch>Vector Addition in PyTorch</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>size</span> <span class=o>=</span> <span class=mi>128</span> <span class=o>*</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty_like</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;PyTorch output:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><ul><li><code>empty_like(a)</code> creates the same size, dtype, and device(&lsquo;cuda&rsquo;) as the input tensor <code>a</code>. It does not initialize the memory into something else, but use the garbage value of it so it&rsquo;s a bit faster than using <code>torch.zeros()</code> or <code>torch.ones()</code>.
The exact operation of vector addition is hidden in operator <code>+</code> in PyTorch.</li></ul><h3 id=vector-addition-in-triton>Vector Addition in Triton</h3><p><a href=https://github.com/triton-lang/triton>Triton</a> is an open source library ran by OpenAI, which aims to be easier to code than CUDA (fewer knobs to control, don&rsquo;t need to know as deep as CUDA) but doesn&rsquo;t lose the performance.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton.language</span> <span class=k>as</span> <span class=nn>tl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@triton.jit</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>add_kernel</span><span class=p>(</span><span class=n>x_ptr</span><span class=p>,</span> <span class=n>y_ptr</span><span class=p>,</span> <span class=n>output_ptr</span><span class=p>,</span> <span class=n>n_elements</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>:</span> <span class=n>tl</span><span class=o>.</span><span class=n>constexpr</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>pid</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>block_start</span> <span class=o>=</span> <span class=n>pid</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span>
</span></span><span class=line><span class=cl>	<span class=n>offsets</span> <span class=o>=</span> <span class=n>block_start</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>mask</span> <span class=o>=</span> <span class=n>offsets</span> <span class=o>&lt;</span> <span class=n>n_elements</span>
</span></span><span class=line><span class=cl>	<span class=n>x</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>x_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>y</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>y_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>output</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>	<span class=n>tl</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>output_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>triton_add</span><span class=p>(</span><span class=n>a</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>b</span><span class=p>:</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty_like</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>n_elements</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>BLOCK_SIZE</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl>	<span class=n>grid</span> <span class=o>=</span> <span class=p>(</span><span class=n>triton</span><span class=o>.</span><span class=n>cdiv</span><span class=p>(</span><span class=n>n_elements</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>),)</span>
</span></span><span class=line><span class=cl>	<span class=n>add_kernel</span><span class=p>[</span><span class=n>grid</span><span class=p>](</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>n_elements</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=o>=</span><span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>size</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>triton_add</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Triton output&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><p>In order to understand this, we need to understand memories and parallel programming. Since this lab is just a tiny experiment, we will not go deep into all the concepts but rather explain what is happening here.</p><p><code>triton_add</code></p><ul><li><code>n_elements = output.numel()</code> calculates the total work to do based on the output size which is 128 in this case.</li><li>We set <code>BLOCK_SIZE</code>, which defines the size of the data to process.</li><li><code>grid</code> calculates how much pids are needed to execute the elements according to BLOCK_SIZE. (In this case it&rsquo;s 128/128 so we only need a single program to execute vector addition) CPU then sends the instruction to GPU to proceed vector addition.<ul><li>We will talk about why the block size is set to 128 specifically, later in some other post (!TODO: I will link that part to here!)</li><li>cdiv is used to include the leftovers after dividing into BLOCK_SIZE. (eg. 130 dimentions / BLOCK_SIZE will result in 2 instead of 1)</li></ul></li></ul><p><code>add_kernel</code></p><ul><li><strong><code>@triton.jit</code></strong> is a special decorator that makes the function into machine language(or kernel) that can be run in GPU. This means unlike <code>triton_add</code> function, in <code>add_kernel</code>, we are using GPU programming. jit is short for just-in-time, which means the code compiles just in time as it runs.</li><li>As you can see in the parameter, we use pointers for inputs and output (<code>x_ptr</code>, <code>y_ptr</code>, and <code>output_ptr</code>). This is because we load data from the GPU RAM.</li><li><code>pid</code> or program ids are unique ids(eg. 0, 1, 2&mldr;) given to the GPU. Each program(kernel) checks its id.</li><li>each program uses its id(eg. 1) and calculate it with the amount of work it should proceed defined by BLOCK_SIZE. (eg. pid=1 should start from <code>1*128</code> to <code>2*128-1</code>)</li><li>Each programs parallely proceeds the process(in this example, it would be vector addition).</li><li>mask helps to check if the offsets do not exceed the actual data range.</li><li>Now, we take inputs from their pointers and <strong>load</strong> data based on the offests and mask.</li><li>The vector addition happens in the ALU in GPU.</li><li>Then we save the result to the output_ptr in GPU RAM.</li></ul><p>first tried to naively check the performance with shell&rsquo;s <code>time</code> command but figured out it was an inappropriate tool to check the actual performance between two codes.</p><p><code>time</code> command measures multiple things in the environment such as Python interpreter starting time, loading libraries, CUDA context initializing (which takes a lot longer than the actual vector addition), and the GPU operation.</p><p>The more accurate way to check performance is to measure using <code>torch.cuda.Event</code></p><p>I made a simple benchmark.py to measure the difference:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>add_triton</span> <span class=kn>import</span> <span class=n>triton_add</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>benchmark_pytorch</span><span class=p>(</span><span class=n>a</span><span class=p>,</span><span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>run_benchmark</span><span class=p>(</span><span class=n>fn</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>start_event</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>Event</span><span class=p>(</span><span class=n>enable_timing</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>end_event</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>Event</span><span class=p>(</span><span class=n>enable_timing</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Warm-up for GPU</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>fn</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>start_event</span><span class=o>.</span><span class=n>record</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>fn</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>end_event</span><span class=o>.</span><span class=n>record</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>elapsed_time_ms</span> <span class=o>=</span> <span class=n>start_event</span><span class=o>.</span><span class=n>elapsed_time</span><span class=p>(</span><span class=n>end_event</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>elapsed_time_ms</span> <span class=o>/</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span>
</span></span><span class=line><span class=cl><span class=n>size</span> <span class=o>=</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pytorch_time</span> <span class=o>=</span> <span class=n>run_benchmark</span><span class=p>(</span><span class=n>benchmark_pytorch</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>triton_time</span> <span class=o>=</span> <span class=n>run_benchmark</span><span class=p>(</span><span class=n>triton_add</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Vector size: </span><span class=si>{</span><span class=n>size</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;PyTorch average time: </span><span class=si>{</span><span class=n>pytorch_time</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2> ms&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Triton average time: </span><span class=si>{</span><span class=n>triton_time</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2> ms&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><code>torch.cuda.synchronize()</code> is used to force CPU to <strong>wait</strong> until GPU operation is done. Since the default behavior between CPU and GPU are asynchronous, we use this command to check the precise amount of time of the GPU operation.</p><p>I&rsquo;ve tried with size = 128 but it was so short the noise took too much portion so I increased the size into <code>1024 * 1024</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>❯ python benchmark.py
</span></span><span class=line><span class=cl>Vector size: <span class=m>1048576</span>
</span></span><span class=line><span class=cl>PyTorch average time: 0.124672 ms
</span></span><span class=line><span class=cl>Triton average time: 0.124037 ms
</span></span></code></pre></div><p>result was almost the same.
We can conclude that vector addition is so simple + PyTorch optimized it well that there seems no room for optimizing vector addition better than PyTorch. PyTorch is as good.</p><p>I guess we will have to cover things computationally heavier, such as matmul.</p><p>Source code can be found in <a href=https://github.com/junuxyz/labs/tree/main/lab_01>https://github.com/junuxyz/labs/tree/main/lab_01</a></p></div><nav class=post-nav><div class=nav-prev><a href=https://junuxyz.github.io/posts/a-shallow-dive-into-vscode-debugger/ class=nav-link><span class=nav-label>← Previous</span>
<span class=nav-title>A (shallow) Dive into VSCode Debugger</span></a></div></nav></article></div></main><footer class=footer><div class=footer-content><p>&copy; 2025 junuxyz. Built with <a href=https://gohugo.io/>Hugo</a>.</p></div></footer><script src=/js/theme.js></script></body></html>