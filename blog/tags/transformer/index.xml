<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer on jlog</title>
    <link>https://junuxyz.github.io/blog/tags/transformer/</link>
    <description>Recent content in Transformer on jlog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Sep 2025 07:39:43 +0900</lastBuildDate>
    <atom:link href="https://junuxyz.github.io/blog/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Shaped Transformer</title>
      <link>https://junuxyz.github.io/blog/posts/shaped-transformer/</link>
      <pubDate>Mon, 15 Sep 2025 07:39:43 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/shaped-transformer/</guid>
      <description>&lt;h2 id=&#34;0-understanding-transformer&#34;&gt;0. Understanding Transformer&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;How can one learn Transformer?&lt;/strong&gt;&lt;br&gt;&#xA;The Transformer Architecture (introduced in the paper &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention is All You Need&lt;/a&gt;&lt;/em&gt;) is one of the most successful models in deep learning and the backbone of what made the “ChatGPT moment” possible. Because of its importance and impact, there are already many high-quality explanations of &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;what the model is&lt;/a&gt;, &lt;a href=&#34;https://www.deeplearning.ai/short-courses/how-transformer-llms-work/&#34;&gt;how it works&lt;/a&gt;, and even &lt;a href=&#34;https://nlp.seas.harvard.edu/annotated-transformer/&#34;&gt;annotated code implementation of it&lt;/a&gt;. These days, most developers don’t need to implement Transformers from scratch because libraries like &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;HuggingFace Transformers&lt;/a&gt; provide easy-to-use classes and methods. Yes, there are plenty of things to build on top of the architecture! Still, I think it is worth having a great understanding of Transformer Model, beyond intuitive, abstract level. In fact one of the best way to learn Transformer, as &lt;a href=&#34;https://www.goodreads.com/quotes/7306651-what-i-cannot-build-i-do-not-understand&#34;&gt;Feynman said&lt;/a&gt;, is to build one yourself from scratch to really understand and appreciate all the underlying techniques and modules that form the base of the ChatGPT era.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
