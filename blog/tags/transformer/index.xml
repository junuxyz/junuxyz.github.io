<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer on jlog</title>
    <link>https://junuxyz.github.io/blog/tags/transformer/</link>
    <description>Recent content in Transformer on jlog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Jan 2026 22:09:17 +0900</lastBuildDate>
    <atom:link href="https://junuxyz.github.io/blog/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Prefill vs Decode</title>
      <link>https://junuxyz.github.io/blog/posts/prefill-vs-decode/</link>
      <pubDate>Fri, 02 Jan 2026 22:09:17 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/prefill-vs-decode/</guid>
      <description>&lt;h2 id=&#34;two-major-operations-in-modern-llm-inference&#34;&gt;Two major operations in modern LLM inference&lt;/h2&gt;&#xA;&lt;p&gt;Two major operations in modern LLM inference are prefill and decode. It’s important to know what they are and how they differ. Most modern LLMs are variations of GPT, which is Decoder-only model. These models take an input (or prompt), process it, sample the next token, and use the previous input + sampled token as the next input in an autoregressive manner. This is called “decoding” and occurs token-by-token.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Shaped Transformer</title>
      <link>https://junuxyz.github.io/blog/posts/shaped-transformer/</link>
      <pubDate>Mon, 15 Sep 2025 07:39:43 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/shaped-transformer/</guid>
      <description>&lt;h1 id=&#34;understanding-transformer&#34;&gt;Understanding Transformer&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;How can one learn Transformer?&lt;/strong&gt;&lt;br&gt;&#xA;The Transformer Architecture (introduced in the paper &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention is All You Need&lt;/a&gt;&lt;/em&gt;) is one of the most successful models in deep learning and the backbone of what made the “ChatGPT moment” possible. Because of its importance and impact, there are already many high-quality explanations of &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;what the model is&lt;/a&gt;, &lt;a href=&#34;https://www.deeplearning.ai/short-courses/how-transformer-llms-work/&#34;&gt;how it works&lt;/a&gt;, and even &lt;a href=&#34;https://nlp.seas.harvard.edu/annotated-transformer/&#34;&gt;annotated code implementation of it&lt;/a&gt;. These days, most developers don’t need to implement Transformers from scratch because libraries like &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;HuggingFace Transformers&lt;/a&gt; provide easy-to-use classes and methods. Yes, there are plenty of things to build on top of the architecture! Still, I think it is worth having a great understanding of Transformer Model, beyond intuitive, abstract level. In fact one of the best way to learn Transformer, as &lt;a href=&#34;https://www.goodreads.com/quotes/7306651-what-i-cannot-build-i-do-not-understand&#34;&gt;Feynman said&lt;/a&gt;, is to build one yourself from scratch to really understand and appreciate all the underlying techniques and modules that form the base of the ChatGPT era.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
