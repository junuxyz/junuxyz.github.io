<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inference on jlog</title>
    <link>https://junuxyz.github.io/blog/tags/inference/</link>
    <description>Recent content in Inference on jlog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Jan 2026 22:09:17 +0900</lastBuildDate>
    <atom:link href="https://junuxyz.github.io/blog/tags/inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Prefill vs Decode</title>
      <link>https://junuxyz.github.io/blog/posts/prefill-vs-decode/</link>
      <pubDate>Fri, 02 Jan 2026 22:09:17 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/prefill-vs-decode/</guid>
      <description>&lt;h2 id=&#34;two-major-operations-in-modern-llm-inference&#34;&gt;Two major operations in modern LLM inference&lt;/h2&gt;&#xA;&lt;p&gt;Two major operations in modern LLM inference are prefill and decode. It’s important to know what they are and how they differ. Most modern LLMs are variations of GPT, which is Decoder-only model. These models take an input (or prompt), process it, sample the next token, and use the previous input + sampled token as the next input in an autoregressive manner. This is called “decoding” and occurs token-by-token.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
