<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Short on jlog</title>
    <link>https://junuxyz.github.io/blog/tags/short/</link>
    <description>Recent content in Short on jlog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Sep 2025 11:13:40 +0900</lastBuildDate>
    <atom:link href="https://junuxyz.github.io/blog/tags/short/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Limitations of Current RL</title>
      <link>https://junuxyz.github.io/blog/posts/limitations-of-current-rl/</link>
      <pubDate>Wed, 03 Sep 2025 11:13:40 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/limitations-of-current-rl/</guid>
      <description>&lt;p&gt;Just read the paper &lt;em&gt;Welcome to the Era of Experience&lt;/em&gt; by Richard S. Sutton and David Silver, and while I admit the potential impact RL will have, Iâ€™m pretty concerned about what these authors believe or are trying to create.&lt;/p&gt;&#xA;&lt;p&gt;I believe AI in general needs to be controlled and understood by humans as much as possible, especially for making important and impactful judgments. However, RL lacks this understandability and controllability because of its unexplainable, black-box decision mechanism. It simply makes choices that will gain the maximum reward based on the reward function we define.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Engineering Era of AI</title>
      <link>https://junuxyz.github.io/blog/posts/the-engineering-era-of-ai/</link>
      <pubDate>Sat, 30 Aug 2025 14:02:26 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/the-engineering-era-of-ai/</guid>
      <description>&lt;p&gt;I feel like the trend of AI is shifting from model construct/pre-training to post-training/pipelining/engineering phase, which we use system-level knowledge to implement the model the fastest, efficient as possible.&lt;/p&gt;&#xA;&lt;p&gt;Of course new models/architectures(world models, System 2, GFlowNet, JEPA etc.), domains(robotics, biology etc.), and paradigms(Reinforcement Learning, Energy based models etc) will come but after all, the &amp;ldquo;verification&amp;rdquo; phase of AI seems done after the huge success and impact of ChatGPT.&lt;/p&gt;&#xA;&lt;p&gt;Now deployment to real world use is getting larger and larger. Big tech companies and AI startups are already noticing and are working on this (&lt;a href=&#34;https://openai.com/index/triton/&#34;&gt;more efficient LLM inference&lt;/a&gt;, faster and better image/video generation (e.g. &lt;a href=&#34;https://blog.google/products/gemini/updated-image-editing-model/&#34;&gt;Nano Banana&lt;/a&gt;), &lt;a href=&#34;https://developer.nvidia.com/isaac/lab&#34;&gt;Robot Learning simulation&lt;/a&gt;, and &lt;a href=&#34;https://www.meta.com/kr/en/ai-glasses/&#34;&gt;smart glasses&lt;/a&gt;) a lot on this. Still I think A LOT MORE are about to be deployed in various fields in various forms in the upcoming years.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The I/O and Pipelining Era of ML</title>
      <link>https://junuxyz.github.io/blog/posts/the-i/o-and-pipelining-era-of-ml/</link>
      <pubDate>Fri, 01 Aug 2025 16:00:15 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/the-i/o-and-pipelining-era-of-ml/</guid>
      <description>&lt;p&gt;Back in the days, constructing and finding novel neural networks(like CNN, RNN, GAN and many more) and scaling it to become &amp;ldquo;deeper&amp;rdquo; was the trend in Deep Learning research.&lt;/p&gt;&#xA;&lt;p&gt;After Transformers came out and as researchers noticed the power of Transformers, I feel the research trend shifted a lot into industrial and engineering problems. Yes, there were and are still some researches focusing on new architectures (like &lt;a href=&#34;https://arxiv.org/abs/2312.00752&#34;&gt;Mamba&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2501.00663&#34;&gt;Titans&lt;/a&gt; etc) but in general I feel the trend has changed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Trying Out uv as my new package managing tool</title>
      <link>https://junuxyz.github.io/blog/posts/trying-out-uv-as-my-new-package-managing-tool/</link>
      <pubDate>Sun, 20 Jul 2025 00:06:57 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/trying-out-uv-as-my-new-package-managing-tool/</guid>
      <description>&lt;p&gt;Dependency hell is a known problem in Machine Learning ecosystem. Hardware(eg. NVIDIA RTX chips) with major libraries such as PyTorch, NumPy etc. can easily create all sorts of dependency issues. That is why making a system for maximum reproducability is important.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve previously used poetry or conda for package managing for Python but found it hard and clunky to use sometimes. Recently I&amp;rsquo;ve found a rising tool for package managing called &lt;a href=&#34;https://github.com/astral-sh/uv/&#34;&gt;uv&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
