<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML on jlog</title>
    <link>https://junuxyz.github.io/blog/categories/ml/</link>
    <description>Recent content in ML on jlog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Sep 2025 07:39:43 +0900</lastBuildDate>
    <atom:link href="https://junuxyz.github.io/blog/categories/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Shaped Transformer</title>
      <link>https://junuxyz.github.io/blog/posts/shaped-transformer/</link>
      <pubDate>Mon, 15 Sep 2025 07:39:43 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/shaped-transformer/</guid>
      <description>&lt;h2 id=&#34;0-understanding-transformer&#34;&gt;0. Understanding Transformer&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;How can one learn Transformer?&lt;/strong&gt;&#xA;The Transformer Architecture (introduced in the paper &lt;em&gt;Attention is All You Need&lt;/em&gt;) is one of the most successful models in deep learning and the backbone of what made the “ChatGPT moment” possible. Because of its importance and impact, there are already many high-quality explanations of what the model is, how it works, and even annotated code implementations. These days, most developers don’t need to implement Transformers from scratch because libraries like HuggingFace provide easy-to-use classes and methods. There are plenty of things to build on top of the architecture! Still, I think it is worth implementing a Transformer from scratch at least once, to really understand and appreciate the techniques that form the base of the ChatGPT era.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Paper Review] UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representation</title>
      <link>https://junuxyz.github.io/blog/posts/paper-review-uniskill-imitating-human-videos-via-cross-embodiment-skill-representation/</link>
      <pubDate>Tue, 26 Aug 2025 13:04:39 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/paper-review-uniskill-imitating-human-videos-via-cross-embodiment-skill-representation/</guid>
      <description>&lt;img src=&#34;https://junuxyz.github.io/images/unkskill.png&#34; alt=&#34;Image&#34;&gt;&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;&#xA;&lt;p&gt;Imitating experts is challenging due to visual, physical differences between human and robot.&lt;/p&gt;&#xA;&lt;p&gt;Previous methods used cross-embodiment datasets with shared scenes and tasks but these data are limited which makes it hard to scale.&lt;/p&gt;&#xA;&lt;p&gt;This paper presents a new framework called &lt;strong&gt;UniSkill&lt;/strong&gt; that learns embodiment-agnostic skill representation from large video dataset.&lt;/p&gt;&#xA;&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;UniSkill uses image-editing pipeline for the neural network to focus on capturing the dynamics changes (over static content) between temproally distant video frames.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lab02: Deploying DialoGPT-Medium with FastAPI &amp; Docker</title>
      <link>https://junuxyz.github.io/blog/posts/lab02-deploying-dialogpt-medium-with-fastapi-docker/</link>
      <pubDate>Wed, 20 Aug 2025 15:57:15 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/lab02-deploying-dialogpt-medium-with-fastapi-docker/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In Lab02, We are going to look at how Jupyter/Colab Notebook models, codes are actually deployed as a service.&lt;/p&gt;&#xA;&lt;img src=&#34;https://junuxyz.github.io/images/simple-chat.png&#34; alt=&#34;Image&#34;&gt;&lt;p&gt;This is a simple chatbot called &lt;em&gt;simple chat&lt;/em&gt; which is containerized in Docker and run on FastAPI.&lt;/p&gt;&#xA;&lt;p&gt;We will build and deploy this chatbot using DialoGPT as the model, use FastAPI to create API endpoints, and deploy it with Docker.&lt;/p&gt;&#xA;&lt;p&gt;The goal of this lab is to experience the end-to-end of deploying and serving a LLM model from scratch, with minimal configuration.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fluent Python Cheat Sheet for Newbies</title>
      <link>https://junuxyz.github.io/blog/posts/fluent-python-cheat-sheet-for-newbies/</link>
      <pubDate>Tue, 22 Jul 2025 18:50:06 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/fluent-python-cheat-sheet-for-newbies/</guid>
      <description>&lt;p&gt;High-level ML frameworks and libraries (e.g., PyTorch, JAX, TensorFlow, NumPy, Triton, and many more) are mostly based on Python.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve known Python for a while, but I&amp;rsquo;ve never learned it to a professional degree and wouldn&amp;rsquo;t say I&amp;rsquo;m good at Python programming. So, I decided to read &lt;em&gt;Fluent Python&lt;/em&gt; (which seems to be one of the &amp;lsquo;bible&amp;rsquo; figures of Python) to cover some topics and improve my Python programming skills.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lab01: Adding Vector</title>
      <link>https://junuxyz.github.io/blog/posts/lab01-adding-vector/</link>
      <pubDate>Sun, 20 Jul 2025 16:26:31 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/lab01-adding-vector/</guid>
      <description>&lt;p&gt;This is a simple experiment to just get a feel of the abstraction PyTorch provides, and all the internal complexity hidden below. We will also compare the performance of basic vector addition between PyTorch, Triton, and CUDA.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Note: This experiment was done in NVIDIA RTX 3050ti laptop GPU&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;vector-addition-in-pytorch&#34;&gt;Vector Addition in PyTorch&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;empty_like&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;PyTorch output:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;empty_like(a)&lt;/code&gt; creates the same size, dtype, and device(&amp;lsquo;cuda&amp;rsquo;) as the input tensor &lt;code&gt;a&lt;/code&gt;. It does not initialize the memory into something else, but use the garbage value of it so it&amp;rsquo;s a bit faster than using &lt;code&gt;torch.zeros()&lt;/code&gt; or &lt;code&gt;torch.ones()&lt;/code&gt;.&#xA;The exact operation of vector addition is hidden in operator &lt;code&gt;+&lt;/code&gt; in PyTorch.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;vector-addition-in-triton&#34;&gt;Vector Addition in Triton&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/triton-lang/triton&#34;&gt;Triton&lt;/a&gt; is an open source library ran by OpenAI, which aims to be easier to code than CUDA (fewer knobs to control, don&amp;rsquo;t need to know as deep as CUDA) but doesn&amp;rsquo;t lose the performance. Check out more information via &lt;a href=&#34;https://openai.com/index/triton/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Trying Out uv as my new package managing tool</title>
      <link>https://junuxyz.github.io/blog/posts/trying-out-uv-as-my-new-package-managing-tool/</link>
      <pubDate>Sun, 20 Jul 2025 00:06:57 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/trying-out-uv-as-my-new-package-managing-tool/</guid>
      <description>&lt;p&gt;Dependency hell is a known problem in Machine Learning ecosystem. Hardware(eg. NVIDIA RTX chips) with major libraries such as PyTorch, NumPy etc. can easily create all sorts of dependency issues. That is why making a system for maximum reproducability is important.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve previously used poetry or conda for package managing for Python but found it hard and clunky to use sometimes. Recently I&amp;rsquo;ve found a rising tool for package managing called &lt;a href=&#34;https://github.com/astral-sh/uv/&#34;&gt;uv&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A (shallow) Dive into VSCode Debugger</title>
      <link>https://junuxyz.github.io/blog/posts/a-shallow-dive-into-vscode-debugger/</link>
      <pubDate>Wed, 16 Jul 2025 21:45:17 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/a-shallow-dive-into-vscode-debugger/</guid>
      <description>&lt;p&gt;I know debugging skills are very important and one of the &amp;ldquo;must have&amp;rdquo; skills for developers. However I did not explicitly tried to learn how to use and utilize VSCode debugger effectively. While reading &lt;a href=&#34;https://www.learncpp.com/cpp-tutorial/using-an-integrated-debugger-stepping/&#34;&gt;this&lt;/a&gt; during my entry to c++, I thought now was the right time to look into features VS Code gives, which were worth note taking. Today is just a shallow dive and hope to learn deeper when I need it.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
