<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>The I/O and Pipelining Era of ML - jlog</title><meta name=description content="Personal blog and thoughts"><meta name=author content="Junu"><meta property="og:url" content="https://junuxyz.github.io/blog/posts/the-i/o-and-pipelining-era-of-ml/"><meta property="og:site_name" content="jlog"><meta property="og:title" content="The I/O and Pipelining Era of ML"><meta property="og:description" content="Back in the days, constructing and finding novel neural networks(like CNN, RNN, GAN and many more) and scaling it to become “deeper” was the trend in Deep Learning research.
After Transformers came out and as researchers noticed the power of Transformers, I feel the research trend shifted a lot into industrial and engineering problems. Yes, there were and are still some researches focusing on new architectures (like Mamba, Titans etc) but in general I feel the trend has changed."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-08-01T16:00:15+09:00"><meta property="article:modified_time" content="2025-08-01T16:00:15+09:00"><meta property="article:tag" content="Short"><meta name=twitter:card content="summary"><meta name=twitter:title content="The I/O and Pipelining Era of ML"><meta name=twitter:description content="Back in the days, constructing and finding novel neural networks(like CNN, RNN, GAN and many more) and scaling it to become “deeper” was the trend in Deep Learning research.
After Transformers came out and as researchers noticed the power of Transformers, I feel the research trend shifted a lot into industrial and engineering problems. Yes, there were and are still some researches focusing on new architectures (like Mamba, Titans etc) but in general I feel the trend has changed."><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=/css/variables.css><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=stylesheet href=/css/custom.css><link rel=icon type=image/x-icon href=/favicon.ico></head><body class=blog-section><header class=header><div class=header-content><div class=logo><a href=/blog/>jlog</a></div><button class=hamburger-menu aria-label="Toggle menu" aria-expanded=false>
<span></span>
<span></span>
<span></span></button><div class=header-right><nav class=nav><a href=/blog/categories/thoughts/ class=nav-item>Thoughts</a>
<a href=/blog/categories/ml/ class=nav-item>ML</a>
<a href=/ class=nav-item>About</a>
<a href=/blog/tags/ class=nav-item>Tags</a></nav></div></div></header><main class=main><div class=container><article class=post><header class=post-header><h1 class=post-title>The I/O and Pipelining Era of ML</h1><div class=post-meta><time datetime=2025-08-01T16:00:15+09:00>August 1, 2025</time><div class=post-tags><a href=/blog/tags/short/ class=tag>#short</a></div></div></header><div class=post-content><p>Back in the days, constructing and finding novel neural networks(like CNN, RNN, GAN and many more) and scaling it to become &ldquo;deeper&rdquo; was the trend in Deep Learning research.</p><p>After Transformers came out and as researchers noticed the power of Transformers, I feel the research trend shifted a lot into industrial and engineering problems. Yes, there were and are still some researches focusing on new architectures (like <a href=https://arxiv.org/abs/2312.00752>Mamba</a>, <a href=https://arxiv.org/abs/2501.00663>Titans</a> etc) but in general I feel the trend has changed.</p><p>Especially after the &ldquo;ChatGPT moment&rdquo; researchers are working on how to efficiently optimize and deploy transformers to serve them in low cost, low latency, and high accuracy. I am not sure if this will be a short term trend after another novel state-of-the-art architecture comes out or another paradigm appears beyond LLMs as <a href=https://x.com/ylecun/status/1911604721267114206>Yann LeCun said</a> (e.g. World Model, Robotics etc).</p><p>I feel at least in the near term the trend of efficiency and engineering will prevail.<br>Frameworks like <a href=https://github.com/sgl-project/sglang>SGLang</a>, <a href=https://github.com/vllm-project/vllm>vLLM</a> and optimization techniques of Transformers such as <a href=https://arxiv.org/abs/2309.06180>PagedAttention</a>, <a href=https://github.com/Dao-AILab/flash-attention>FlashAttention</a> and tools like <a href=https://github.com/triton-lang/triton>Triton</a> and <a href=https://en.wikipedia.org/wiki/CUDA>CUDA</a> programming are getting much traction than few years before.</p><p>If I am thinking in the right direction, the mental model should be focused on <strong>I/O and pipelining</strong>.</p><p>This means we need to understand</p><ul><li>how each process of training and deploying are done</li><li>why does each steps even exist (can we reduce the steps?)</li><li>what inputs produces what outputs</li><li>identify the underlying bottleneck (cost of time or latency etc) and optimize it.</li></ul></div><nav class=post-nav><div class=nav-prev><a href=/blog/posts/what-i-like-and-what-i-dont/ class=nav-link><span class=nav-label>previous: </span><span class=nav-title>What I Like and What I Don't</span></a></div><div class=nav-next><a href=/blog/posts/ideas-im-interested-in-ai-research-field/ class=nav-link><span class=nav-label>next: </span><span class=nav-title>Ideas I'm interested in AI (Research & Field)</span></a></div></nav></article></div></main><footer class=footer><div class=footer-content><p>&copy; 2026 jlog. Built with <a href=https://gohugo.io/>Hugo</a>.</p><div class=social-icons><a href=https://github.com/junuxyz class=social-icon aria-label=GitHub target=_blank rel=noopener>GitHub
</a><a href=mailto:jpjunior211@gmail.com class=social-icon aria-label=Email target=_blank rel=noopener>Email</a></div></div></footer><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".hamburger-menu"),e=document.querySelector(".header-right");if(t&&e){t.addEventListener("click",function(){const t=this.getAttribute("aria-expanded")==="true";this.setAttribute("aria-expanded",!t),e.classList.toggle("active")});const n=e.querySelectorAll(".nav-item");n.forEach(n=>{n.addEventListener("click",function(){t.setAttribute("aria-expanded","false"),e.classList.remove("active")})}),document.addEventListener("click",function(n){!e.contains(n.target)&&!t.contains(n.target)&&(t.setAttribute("aria-expanded","false"),e.classList.remove("active"))})}document.querySelectorAll("pre code").forEach(e=>{const t=e.parentElement;if(!t.querySelector(".code-copy-btn")){const n=document.createElement("button");n.className="code-copy-btn",n.setAttribute("aria-label","Copy code to clipboard");const s=`<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <rect x="5" y="5" width="8" height="8" rx="1" stroke="currentColor" stroke-width="1.5" fill="none"/>
                        <path d="M4 3C4 2.44772 4.44772 2 5 2H9C9.55228 2 10 2.44772 10 3V4H5C4.44772 4 4 4.44772 4 5V11C4 11.5523 4.44772 12 5 12H6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round"/>
                    </svg>`,o=`<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M13.5 4.5L6 12L2.5 8.5" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"/>
                    </svg>`;n.innerHTML=s,n.addEventListener("click",()=>{navigator.clipboard.writeText(e.textContent).then(()=>{n.innerHTML=o,n.classList.add("copied"),setTimeout(()=>{n.innerHTML=s,n.classList.remove("copied")},2e3)})}),t.style.position="relative",t.appendChild(n)}if(!t.querySelector(".code-language")){let n=e.getAttribute("data-lang");if(!n){const t=(e.className||"").match(/language-([^\s]+)/);t&&t[1]&&(n=t[1])}if(n){const e=document.createElement("span");e.className="code-language",e.textContent=n.charAt(0).toUpperCase()+n.slice(1),t.appendChild(e)}}})})</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></body></html>