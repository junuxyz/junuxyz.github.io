<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on jlog</title>
    <link>https://junuxyz.github.io/blog/posts/</link>
    <description>Recent content in Posts on jlog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Jan 2026 22:09:17 +0900</lastBuildDate>
    <atom:link href="https://junuxyz.github.io/blog/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Prefill vs Decode</title>
      <link>https://junuxyz.github.io/blog/posts/prefill-vs-decode/</link>
      <pubDate>Fri, 02 Jan 2026 22:09:17 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/prefill-vs-decode/</guid>
      <description>&lt;h2 id=&#34;two-major-operations-in-modern-llm-inference&#34;&gt;Two major operations in modern LLM inference&lt;/h2&gt;&#xA;&lt;p&gt;Two major operations in modern LLM inference are prefill and decode. It’s important to know what they are and how they differ. Most modern LLMs are variations of GPT, which is Decoder-only model. These models take an input (or prompt), process it, sample the next token, and use the previous input + sampled token as the next input in an autoregressive manner. This is called “decoding” and occurs token-by-token.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Shaped Transformer</title>
      <link>https://junuxyz.github.io/blog/posts/shaped-transformer/</link>
      <pubDate>Mon, 15 Sep 2025 07:39:43 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/shaped-transformer/</guid>
      <description>&lt;h1 id=&#34;understanding-transformer&#34;&gt;Understanding Transformer&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;How can one learn Transformer?&lt;/strong&gt;&lt;br&gt;&#xA;The Transformer Architecture (introduced in the paper &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention is All You Need&lt;/a&gt;&lt;/em&gt;) is one of the most successful models in deep learning and the backbone of what made the “ChatGPT moment” possible. Because of its importance and impact, there are already many high-quality explanations of &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;what the model is&lt;/a&gt;, &lt;a href=&#34;https://www.deeplearning.ai/short-courses/how-transformer-llms-work/&#34;&gt;how it works&lt;/a&gt;, and even &lt;a href=&#34;https://nlp.seas.harvard.edu/annotated-transformer/&#34;&gt;annotated code implementation of it&lt;/a&gt;. These days, most developers don’t need to implement Transformers from scratch because libraries like &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;HuggingFace Transformers&lt;/a&gt; provide easy-to-use classes and methods. Yes, there are plenty of things to build on top of the architecture! Still, I think it is worth having a great understanding of Transformer Model, beyond intuitive, abstract level. In fact one of the best way to learn Transformer, as &lt;a href=&#34;https://www.goodreads.com/quotes/7306651-what-i-cannot-build-i-do-not-understand&#34;&gt;Feynman said&lt;/a&gt;, is to build one yourself from scratch to really understand and appreciate all the underlying techniques and modules that form the base of the ChatGPT era.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lab02: Deploying DialoGPT-Medium with FastAPI &amp; Docker</title>
      <link>https://junuxyz.github.io/blog/posts/lab02-deploying-dialogpt-medium-with-fastapi-docker/</link>
      <pubDate>Wed, 20 Aug 2025 15:57:15 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/lab02-deploying-dialogpt-medium-with-fastapi-docker/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In Lab02, We are going to look at how Jupyter/Colab Notebook models, codes are actually deployed as a service.&lt;/p&gt;&#xA;&lt;img src=&#34;https://junuxyz.github.io/images/simple-chat.png&#34; alt=&#34;Image&#34;/&gt;&#xA;&#xA;&lt;p&gt;This is a simple chatbot called &lt;em&gt;simple chat&lt;/em&gt; which is containerized in Docker and run on FastAPI.&lt;/p&gt;&#xA;&lt;p&gt;We will build and deploy this chatbot using DialoGPT as the model, use FastAPI to create API endpoints, and deploy it with Docker.&lt;/p&gt;&#xA;&lt;p&gt;The goal of this lab is to experience the end-to-end of deploying and serving a LLM model from scratch, with minimal configuration.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lab01: Adding Vector</title>
      <link>https://junuxyz.github.io/blog/posts/lab01-adding-vector/</link>
      <pubDate>Sun, 20 Jul 2025 16:26:31 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/lab01-adding-vector/</guid>
      <description>&lt;p&gt;This is a simple experiment to just get a feel of the abstraction PyTorch provides, and all the internal complexity hidden below. We will also compare the performance of basic vector addition between PyTorch, Triton, and CUDA.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Note: This experiment was done in NVIDIA RTX 3050ti laptop GPU&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;vector-addition-in-pytorch&#34;&gt;Vector Addition in PyTorch&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;empty_like&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;PyTorch output:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;empty_like(a)&lt;/code&gt; creates the same size, dtype, and device(&amp;lsquo;cuda&amp;rsquo;) as the input tensor &lt;code&gt;a&lt;/code&gt;. It does not initialize the memory into something else, but use the garbage value of it so it&amp;rsquo;s a bit faster than using &lt;code&gt;torch.zeros()&lt;/code&gt; or &lt;code&gt;torch.ones()&lt;/code&gt;.&lt;br&gt;&#xA;The exact operation of vector addition is hidden in operator &lt;code&gt;+&lt;/code&gt; in PyTorch.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;vector-addition-in-triton&#34;&gt;Vector Addition in Triton&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/triton-lang/triton&#34;&gt;Triton&lt;/a&gt; is an open source library ran by OpenAI, which aims to be easier to code than CUDA (fewer knobs to control, don&amp;rsquo;t need to know as deep as CUDA) but doesn&amp;rsquo;t lose the performance. Check out more information via &lt;a href=&#34;https://openai.com/index/triton/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Not Regret</title>
      <link>https://junuxyz.github.io/blog/posts/how-to-not-regret/</link>
      <pubDate>Sat, 28 Jun 2025 09:00:12 +0900</pubDate>
      <guid>https://junuxyz.github.io/blog/posts/how-to-not-regret/</guid>
      <description>&lt;p&gt;Today, I am going to talk about how to not regret. This is important to many of us, since we spend lots of time regretting our past, suffering about the future, and blaming our current selves.&lt;/p&gt;&#xA;&lt;p&gt;The answer to this is quite straightforward: &lt;strong&gt;Do your best in the given situation.&lt;/strong&gt; I know, it sounds predictable and perhaps a bit cliché, but please, keep reading. You think you&amp;rsquo;ve failed and lost too many good chances? It doesn&amp;rsquo;t matter; the answer remains the same: &lt;strong&gt;Do your best in the given situation.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
