<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Shaped Transformer - jlog</title><meta name=description content="Personal blog and thoughts"><meta name=author content="Junu"><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=icon type=image/x-icon href=/favicon.ico></head><body class=blog-section><header class=header><div class=header-content><div class=logo><a href=/blog/>jlog</a></div><div class=header-right><nav class=nav><a href=/blog/categories/thoughts/ class=nav-item>Thoughts</a>
<a href=/blog/categories/ml/ class=nav-item>ML</a>
<a href=/blog/about/ class=nav-item>About</a>
<a href=/blog/tags/ class=nav-item>Tags</a></nav></div></div></header><main class=main><div class=container><article class=post><header class=post-header><h1 class=post-title>Shaped Transformer</h1><div class=post-meta><time datetime=2025-09-15T07:39:43+09:00>September 15, 2025</time><div class=post-tags><a href=/blog/tags/transformer/ class=tag>#Transformer</a></div></div></header><div class=post-content><h2 id=0-understanding-transformer>0. Understanding Transformer</h2><p><strong>How can one learn Transformer?</strong><br>The Transformer Architecture (introduced in the paper <em><a href=https://arxiv.org/abs/1706.03762>Attention is All You Need</a></em>) is one of the most successful models in deep learning and the backbone of what made the “ChatGPT moment” possible. Because of its importance and impact, there are already many high-quality explanations of <a href=https://jalammar.github.io/illustrated-transformer/>what the model is</a>, <a href=https://www.deeplearning.ai/short-courses/how-transformer-llms-work/>how it works</a>, and even <a href=https://nlp.seas.harvard.edu/annotated-transformer/>annotated code implementation of it</a>. These days, most developers don’t need to implement Transformers from scratch because libraries like <a href=https://huggingface.co/docs/transformers/index>HuggingFace Transformers</a> provide easy-to-use classes and methods. Yes, there are plenty of things to build on top of the architecture! Still, I think it is worth having a great understanding of Transformer Model, beyond intuitive, abstract level. In fact one of the best way to learn Transformer, as <a href=https://www.goodreads.com/quotes/7306651-what-i-cannot-build-i-do-not-understand>Feynman said</a>, is to build one yourself from scratch to really understand and appreciate all the underlying techniques and modules that form the base of the ChatGPT era.</p><p><strong>How is this different from other content?</strong><br>Before I start, I do strongly recommend reading other resources as well. However note that each sources has different layers of abstraction (or depth of explanation). The <a href=https://arxiv.org/abs/1706.03762>paper itself</a> is fairly straightforward but not chronologically ordered, so it can be hard to follow in details. <em><a href=https://jalammar.github.io/illustrated-transformer/>The Illustrated Transformer</a></em> is beginner-friendly, abstracting away many implementation details and excels at explaining the overall big picture. On the other hand, <em><a href=https://nlp.seas.harvard.edu/annotated-transformer/>The Annotated Transformer</a></em> is very deep, building the entire architecture end to end in PyTorch. But since it follows the paper’s order (which isn&rsquo;t chronological) and leaves out some explanations, readers who only have an abstract understanding of the model may feel overwhelmed or questionable.</p><p>Also note that Transformer is not a single monolithic block—it’s made up of many modularized layers (tokenization, positional encoding, encoder-decoder model, self-attention, cross-attention, etc.). Unless you already have a solid background in deep learning and NLP, it’s hard to fully understand all the pieces in one go. You’ll often need additional resources, and repeated exposure, to get comfortable with it.</p><p>While there are many great explanations of the mathematics and abstract concepts, I think the end-to-end shape changes and detailed explanation of code implementation are often missing. This blog post specifically aims to enhance the reader’s intuition about what the input actually looks like in real code, how it gets transformed step by step, and how it eventually can successfully predict the “next token”.</p><p>Hopefully this helps you form a more concrete understanding of the architecture and makes the code easier to read and implement :)</p><br><h2 id=1-commonly-used-parameters>1. Commonly Used Parameters</h2><p>Before we talk about shape transformation, it is helpful to understand the names of the parameter/notations. It will help the code readability. If you are famaliar with the paper and the parameters used, feel free to skip this section.</p><h3 id=n--b-or-nbatches>$N$ , $b$, or <code>nbatches</code></h3><p>The Paper use the expression $N$ but in code, it is expressed as <code>nbatches</code>.</p><p>The reason why you may be confused about <code>nbatches</code> in code implementation from Annotated Transformer is because most explanation (including the original paper) omit about it.</p><p>The most representative image of Transformer is usually</p><ol><li>One head from one batch or</li><li>Multi-Head from one batch</li></ol><p>but they don&rsquo;t explicitly tell there are <code>nbatches</code> batches processed parallely for each batch.</p><p>The reality is, $N$ sentences are put into batch and passed as input. But this doesn’t change or introduce anything new to the original architecture we know. Still it&rsquo;s worth noting that <code>nbatches</code> mean the number of sentences being processed per batch. It will appear in the code several times.</p><p>Ex. let’s say $N=3$. That means we have $3$ sentences per batch.</p><h3 id=s-or-n_seq>$S$ or <code>n_seq</code></h3><p><code>n_seq</code> means the number of tokens in one sentence. Since Transformer utilizes parallel processing, we need to pre-define (statically) the length of the sentence. Usually we define it based on the longest sentence.</p><p>For example,</p><pre tabindex=0><code>I love you            --3
I am in love with you --6
I love you, too       --5
</code></pre><p>in this case, since the longest sentence in the batch is $6$, we can set <code>n_seq = 6</code>.<br>For the sentences that have less tokens than 6 will be filled with mask. We will see how mask(padding) is implemented later in this post.</p><h3 id=d_textmodel-or-d_model>$d_{\text{model}}$ or <code>d_model</code></h3><p><code>d_model</code> is the dimensionality of the <strong>residual stream</strong>, i.e., the vector size that represents each token throughout the Transformer.</p><p>All major components, including Embedding, Attention, and Feed-Forward layers, produce outputs of shape <code>(N, S, d_model)</code>. This uniform dimension ensures that residual connections (<code>x = x + sublayer(x)</code>) can be applied seamlessly across all sublayers.</p><p>In the original paper, <code>d_model</code> was set to 512.</p><h3 id=vocab-or-vocab>$vocab$ or <code>vocab</code></h3><p><code>vocab</code> is number of all token (or token ID). It depends on how you tokenize it.</p><p>I think prior resources didn’t explain about the exact input of Transformer architecture clearly but I think it’s worth noting.</p><p>First, even before the transformer process begins, there is a thing called Tokenizer which is independent from the Transformer Architecture. The tokenizer splits raw sentences into seqeunce of tokens.</p><p>For example if the raw sentence input was <code>I love you</code>, Tokenizer would divide it into tokens,</p><pre tabindex=0><code>[&#34;I&#34;, &#34;love&#34;, &#34;you&#34;]
</code></pre><p>and using the $vocab$ dictionary, we map the tokens with its correspoining token id (one-on-one match)</p><pre tabindex=0><code>[0, 1, 2]
</code></pre><p>Now <strong>this (sequences of token id)</strong> is the input of the Transformer Architecture.</p><p>Then you might ask <strong>what&rsquo;s the input of Transformer then?</strong></p><p>The very first step of Transformer is Embedding and this is done by selecting the row from <code>W_emb</code> using the token ID as a key: <code>W_emb[token ID]</code></p><pre tabindex=0><code>token_id = 1 (&#34;love&#34;)
→ W_emb[1] = [0.13, -0.25, 0.91, ..., 0.07] 
</code></pre><p>Since the vector representation of token has size of $d_\text{model}$ (as mentioned above) The shape of $W_\text{emb}$ will be $(vocab, d_{\text{model}})$.</p><p>tldr:</p><ul><li><code>vocab</code> means number of all possible tokens, which depends on the Tokenizer.</li><li>After the token is converted into token id, the sequence of token ids become the actual “input” of the Transformer Architecture (The most left, below from the Transformer Architecture image)</li></ul><h3 id=parameters-specifically-used-during-attention-calculation>Parameters specifically used During Attention Calculation</h3><p>Below are the parameters only seen in Attention Calculation (Self-Attention and Cross-Attention)</p><h3 id=h-h-or-h>$H$, $h$, or <code>h</code></h3><p><code>h</code> is a hyperparameter which means the number of head of Multi-Head Attention.<br>In the original paper, researchers set it as <code>h = 8</code>.</p><h3 id=d_k-or-d_k>$d_k$ or <code>d_k</code></h3><p><code>d_k</code> is the vector size of key($K$) representation of each token.</p><p>We will look into more detail about how shape transforms during Multi-Head Attention in the upcoming section, but to shortly address, $Q$(query) and $K$(key) are matrix multiplied to get the Attention Score. Therefore <code>d_q</code> must be the same as <code>d_k</code>.</p><p>In the paper <code>d_k = d_model // h</code> which is $64$ ($=512/8$). Most people think <code>d_k</code> must be <code>d_model // h</code> but this is just a design choice and totally depends on the developer. I will explain cases where this is not always true, while it&rsquo;s still efficient to use <code>d_k = d_model // h</code>.</p><h3 id=d_v-or-d_v>$d_v$ or <code>d_v</code></h3><p><code>d_v</code> is the dimension of the value($V$) vector for each token.</p><p>After the attention weights are calculated, they are multiplied by the Value matrix $V$. This process yields a new set of vectors, each with dimension <code>d_v</code>, that now holds the contextual information from the sequence. This output is then used to help predict the next token.<br>(Don&rsquo;t worry if this sounds too compact. I will explain it in more detail in the next section!)</p><p>In the original &ldquo;Attention Is All You Need&rdquo; paper, the authors set <code>d_v = d_k = d_q</code>. However, while <code>d_k</code> must equal <code>d_q</code>, it&rsquo;s not required for d_v to be the same size. This is simply another design choice. I will also explain later in this post when <code>d_v != d_k</code> is acceptable.</p><br><h2 id=2-the-big-picture-tldr>2. The Big Picture (TLDR)</h2><p>I wrote this part for people who don&rsquo;t have much time to look into all the details (though I believe it&rsquo;s worth digging in), and also wanted to emphasize the most important part, how everything works in the big picture and how the shape changes.</p><p><strong>0. Input</strong> (For both Encoder and Decoder)<br>Raw sentences in batches are each converted into sequence of tokens. output shape: <code>(nbatches, n_seq, vocab)</code></p><div class="note-block note-info"><div class=note-icon>ℹ️</div><div class=note-content><div class=note-title>Note</div><div class=note-body><ul><li>For differentiation, we will express Encoder input as <code>(nbatches, n_seq_src, vocab)</code> and Decoder input as <code>(nbatches, n_seq_tgt, vocab)</code>.</li><li>For the decoder, we will later also embed the ground-truth output sequence in the same way.</li></ul></div></div></div><p><strong>1. Encoder Embedding Layer</strong><br>1-1. Each tokens are embedded in <code>d_model</code> sized vector. <code>(nbatches, n_seq_src, vocab)</code> -> <code>(nbatches, n_seq_src, d_model)</code><br>1-2. Each token adds positional information through Positional Embedding. Since the size of Positional vector is same(<code>d_model</code>), shape doesn&rsquo;t change. <code>(nbatches, n_seq_src, d_model)</code> -> <code>(nbatches, n_seq_src, d_model)</code>.</p><p><strong>2. Encoder Layer</strong><br>Encoder Layer has two kinds of submodules(2-1. MHA and 2-2. FFN).<br>2-1~ 2-2 is recurred $N$($N=6$ in the original paper) times.</p><p><strong>2-1. Multi-Head Attention</strong><br>The result of 1-2 becomes the input and goes into Multi-Head Attention. input shape: <code>(nbatches, n_seq_src, d_model)</code><br>2-1-1. Copy the input and Project it to <code>W_q</code>, <code>W_k</code>, <code>W_v</code>. Since each weights are all shape of <code>(d_model, d_model)</code>, the shape doesn&rsquo;t change. Q, K, V = <code>(nbatches, n_seq_src, d_model)</code><br>2-1-2. Split into multiple heads. <code>d_k = d_model // h</code>, so we can reshape the model as <code>(nbatches, n_seq_src, d_model)</code> -> <code>(nbatches, n_seq_src, h, d_k)</code><br>2-1-3. Transpose the head for parallel processing. Since we want to process each head&rsquo;s attention process in parallel, we transpose the head and sequence. This makes processing each sequence(sentence) for each head in parallel. <code>(nbatches, n_seq_src, h, d_k)</code> -> <code>(nbatches, h, n_seq_src, d_k)</code><br>2-1-4. Matrix Multiply $QK^T$ <code>(nbatches, h, n_seq_src, d_k)</code> -> <code>(nbatches, h, n_seq_src, n_seq_src)</code><br>2-1-5. Get Attention weights by Softmax. Shape of course, doesn&rsquo;t change. <code>(nbatches, h, n_seq_src, n_seq_src)</code> -> <code>(nbatches, h, n_seq_src, n_seq_src)</code><br>2-1-6. Matrix Multiply with $V$ <code>(nbatches, h, n_seq_src, n_seq_src)</code> -> <code>(nbatches, h, n_seq_src, d_k)</code><br>2-1-7. Now due to 2-1-1~2-1-6 we&rsquo;ve got <code>h</code> numbers of $V$s and we would want to concatenate all the information and represent as one. So we first transpose the head and sequence: <code>(nbatches, h, n_seq_src, d_k)</code> -> <code>(nbatches, n_seq_src, h, d_k)</code><br>2-1-8. We concatenate all the head&rsquo;s results. <code>(nbatches, n_seq_src, h, d_k)</code> -> <code>(nbatches, n_seq_src, h * d_k)</code> which is same as <code>(nbatches, n_seq_src, d_model)</code> since <code>d_model = h * d_k</code></p><p><strong>Residual connection & Layer Normalization</strong><br>2-1-9. Residual connection is a method introduced in <a href=https://arxiv.org/abs/1512.03385>this</a> paper, which is known to make learning fast and better. We won&rsquo;t go into details about this since it&rsquo;s out of the scope of our interest in this post. Check <a href=https://junuxyz.notion.site/Deep-Residual-Learning-for-Image-Recognition-ResNet-2514fc94bcb4816b9efcd9ca9a5eaf68>my note</a> on ResNet for a more detailed explanation. Since <code>x = x + MHA(x)</code> and both shapes are <code>(nbatches, n_seq_src, d_model)</code> the addition also results as <code>(nbatches, n_seq_src, d_model)</code>.<br>2-1-10. Layer Normalization normalizes the elements but doesn&rsquo;t change the shape: <code>(nbatches, n_seq_src, d_model)</code> -> <code>(nbatches, n_seq_src, d_model)</code></p><p><strong>2-2. Feed Forward Network</strong><br>Feed Forward Network is just a 2-Layer MLP with ReLU as the activation function. The design decisions in Transformer architecture is to preserve the <code>d_model</code> dimension space for each steps as much as possible, so while the hidden layer&rsquo;s dimension is increased, the input and output doesn&rsquo;t change in this stage as well. <code>(nbatches, n_seq_src, d_model)</code> -> <code>(nbatches, n_seq_src, d_model)</code></p><p><strong>Residual connection & Layer Normalization</strong><br>I&rsquo;ve already explained about this stage in 2-2. tldr: input and output shape does not change. <code>(nbatches, n_seq_src, d_model)</code> -> <code>(nbatches, n_seq_src, d_model)</code>.</p><p><strong>3. Decoder Embedding Layer</strong></p><div class="note-block note-info"><div class=note-icon>ℹ️</div><div class=note-content><div class=note-title>Note</div><div class=note-body>Decoder (Embedding ~ Decoder Layer) has many similarities compared to Encoder, in general. While I will explain every parts (even duplicate ones) because I want to make the understanding of the process as concrete as possible, I will highlight the parts that differs from Encoder.</div></div></div><p>3-1. Each target tokens we got from <strong>step 0</strong> (ground truth during training, or previously generated tokens during inference) are embedded in <code>d_model</code> sized vector.<br><code>(nbatches, n_seq_tgt, vocab)</code> → <code>(nbatches, n_seq_tgt, d_model)</code><br>3-2. Add positional information through Positional Embedding. Shape does not change.<br><code>(nbatches, n_seq_tgt, d_model)</code> → <code>(nbatches, n_seq_tgt, d_model)</code></p><p><strong>4. Decoder Layer</strong><br>Decoder Layer has three kinds of submodules, which is similar to Encoder Layer but additional Cross Attention Submodule Layer is added in the middle. (4-1. Masked Multi-Head Self Attention, 4-2. Cross Attention, 4-3. Feed Forward Network). As Encoder Layers has, at the end of every submodules are residually connected and layer normalized.<br>4-1~ 4-3 is recurred $N$($N=6$ in the original paper) times.</p><p><strong>4-1. Masked Multi-Head Self Attention</strong><br>Input: <code>(nbatches, n_seq_tgt, d_model)</code><br>Same process as Encoder MHA, but <strong>mask is applied</strong> to prevent each position from attending to future positions(which is considered as <em>cheating</em>). Since we saw the detailed explanation of how shape changes from Encoder Layer, I will quickly skim through the flow of shape change:</p><ul><li>Project to Q, K, V: <code>(nbatches, n_seq_tgt, d_model)</code></li><li>Split heads: <code>(nbatches, n_seq_tgt, h, d_k)</code></li><li>Transpose: <code>(nbatches, h, n_seq_tgt, d_k)</code></li><li>$QK^T$ + mask + softmax: <code>(nbatches, h, n_seq_tgt, n_seq_tgt)</code></li><li>Multiply by V: <code>(nbatches, h, n_seq_tgt, d_k)</code></li><li>Transpose back & concat: <code>(nbatches, n_seq_tgt, d_model)</code></li></ul><p><strong>Residual Connection & LayerNorm</strong></p><ul><li>Add input and normalize: <code>(nbatches, n_seq_tgt, d_model)</code></li></ul><p><strong>4-2. Encoder–Decoder(Cross) Attention</strong><br>This part is what differs the most. The reason why we have this layer is because we want to send the information from the Encoder to the Decoder Layer. Unlike self attention layers where <code>Q,K,V</code> all came from the same source (from either Enocder Input or Decoder Input), Q comes from the decoder output of decoder(4-1) <code>(nbatches, n_seq_tgt, d_model)</code> while K, V come from encoder output(after 2 is finished) <code>(nbatches, n_seq_src, d_model)</code>. Since the shapes are all the same with the source different, the shape changes and calculations are all identical compared to the self attention blocks.</p><ul><li>Shape flow:<ul><li>Project to QKV<ul><li>Q: <code>(nbatches, n_seq_tgt, d_model)</code></li><li>K, V: <code>(nbatches, n_seq_src, d_model)</code></li></ul></li><li>Split heads<ul><li>Q: <code>(nbatches, n_seq_tgt, h, d_k)</code></li><li>K, V: <code>(nbatches, n_seq_src, h, d_k)</code></li></ul></li><li>Transpose<ul><li>Q: <code>(nbatches, h, n_seq_tgt, d_k)</code></li><li>K, V: <code>(nbatches, h, n_seq_src, d_k)</code></li></ul></li><li>$QK^T$: <code>(nbatches, h, n_seq_tgt, d_k)</code> × <code>(nbatches, h, n_seq_src, d_k)</code> → <code>(nbatches, h, n_seq_tgt, n_seq_src)</code></li><li>Multiply by V: <code>(nbatches, h, n_seq_tgt, d_k)</code></li><li>Transpose back & concat: <code>(nbatches, n_seq_tgt, d_model)</code></li></ul></li></ul><p><strong>Residual Connection & LayerNorm</strong></p><ul><li>Add input and normalize: <code>(nbatches, n_seq_tgt, d_model)</code></li></ul><p><strong>4-3. Encoder–Decoder(Cross) Attention</strong><br>Same as <strong>2-2</strong> (2-layer MLP with ReLU in between). The input shape is the same and output doesn&rsquo;t change.<br><code>(nbatches, n_seq_tgt, d_model)</code> → <code>(nbatches, n_seq_tgt, d_model)</code></p><p><strong>Residual Connection & LayerNorm</strong><br><code>(nbatches, n_seq_tgt, d_model)</code> -> <code>(nbatches, n_seq_tgt, d_model)</code></p><p><strong>5. LMHead</strong> (Generator)<br>After Decoder process is done, the decoder&rsquo;s output is projected back into the vocabulary space.<br>5-1. Linear Layer<br>We want to project the <code>d_model</code> for each token into token space <code>vocab</code>. So after passing this to the linear layer the shape changes <code>(nbatches, n_seq_tgt, d_model)</code> -> <code>(nbatches, n_seq_tgt, vocab)</code></p><p>5-2. Softmax<br>We want the model to predict the next token in a stochastic way so we imply softmax function as well.<br><code>(nbatches, n_seq_tgt, d_model)</code> -> <code>(nbatches, n_seq_tgt, vocab)</code></p><hr><p>Section 3 below explains much detailed explanation for each steps with examples, illustrations, and code examples(from <em>Annotated Transformer</em>).</p><br><h2 id=3-how-shape-changes-end-to-end-w-code-examples>3. How Shape Changes, end to end (w/ Code Examples)</h2><div class="note-block note-info"><div class=note-icon>ℹ️</div><div class=note-content><div class=note-title>Note</div><div class=note-body>This section is working in progress!</div></div></div><p>When we encounter the code implementation of Transformer Architecture, all kinds of <code>view()</code>, <code>transpose()</code>, <code>reshape()</code> methods are frequently used, hindering what are being changed , what it implies, and why we need to do it. After all, if you understand there is a general order of the code and the shape form all has its meanings, code readability can significantly enhance.</p><p>Before we start, a simple but effective tip is to remember that most calculations<br>(token embedding, self-attention, feed-forward, etc.) in Transformer are applied <strong>per token</strong>(<code>d_model</code>).<br>In practice, the input shape is <code>(nbatches, n_seq, d_model)</code>, which means each sequence has <code>n_seq</code> tokens and each batch has <code>nbatches</code> sentences. Almost all operations are performed independently on each of these tokens (in parallel). So you can think of it as running the same function <code>nbatches × n_seq</code> times in parallel.</p><p>Now, let&rsquo;s explore the journey from the very first embedding to the last output (next token) and see how the shape changes and what they all mean. (In most paper or images, they often omit <code>nbatches</code> or <code>h</code> for clarity but I will explain including all the parameters). Then we will see how code is actually written to match these shapes. We won&rsquo;t cover all code, just the shape transformation parts, for simplicity. Also, to give you a clear intuition of how everything is working, I will use the three following sentences I used above. Code can be found in &mldr; :</p><h3 id=tokenizing>Tokenizing</h3><p><strong>shape: <code>(nbatches, n_seq)</code></strong></p><p>As mentioned above, this is a step before the transformer architecture even starts. It&rsquo;s a process to convert raw sentences into sequence of tokens. For example, based on <a href=https://platform.openai.com/tokenizer>GPT-4o & GPT-4o mini tokenizer</a> provided by OpenAI,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-Plain data-lang=Plain><span class=line><span class=cl>I love you            
</span></span><span class=line><span class=cl>I am in love with you
</span></span><span class=line><span class=cl>I love you, too
</span></span></code></pre></div><p>(we will use these three sentences in each step until the end of the architecture)<br>are converted into discrete tokens:</p><img src=/images/shaped-trasnformer-example-1.png alt=Image><div class="note-block note-info"><div class=note-icon>ℹ️</div><div class=note-content><div class=note-title>Note</div><div class=note-body>The only reason we use the GPT-4o tokenizer here is since it&rsquo;s the most convenient tokenizer available on the web. However all the rest of the concepts and parameters (e.g. size of $vocab$) will be based on the original paper!</div></div></div><p>In code, we use <code>tokenizer.tokenizer</code> module:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>tokenize</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[</span><span class=n>tok</span><span class=o>.</span><span class=n>text</span> <span class=k>for</span> <span class=n>tok</span> <span class=ow>in</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenizer</span><span class=p>(</span><span class=n>text</span><span class=p>)]</span>
</span></span></code></pre></div><p>We use padding tokens(<code>PAD</code>) to keep the length of all <code>n_seq</code> same (which is crucial for matrix calculation). If we set <code>n_seq</code> to 6, the padding will fill as below:</p><img src=/images/shaped-trasnformer-example-2.png alt=Image><p>These tokens converted to token ids will be</p><img src=/images/shaped-trasnformer-example-3.png alt=Image><p>This will be our exact starting point (input) for Transformer Architecture.</p><p>Let&rsquo;s see how this is implemented as code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Batch</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Object for holding a batch of data with mask during training.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># In this code, padding token(`&lt;blank&gt;`) is set to &#39;2&#39;.</span>
</span></span><span class=line><span class=cl>    <span class=c1># This is just because the authors of Annotated Transformer</span>
</span></span><span class=line><span class=cl>    <span class=c1># set `&lt;blank&gt;` as the third special token (</span>
</span></span><span class=line><span class=cl>    <span class=c1># specials=[&#34;&lt;s&gt;&#34;, &#34;&lt;/s&gt;&#34;, &#34;&lt;blank&gt;&#34;, &#34;&lt;unk&gt;&#34;])</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>pad</span><span class=o>=</span><span class=mi>2</span><span class=p>):</span>  <span class=c1># 2 = &lt;blank&gt;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>src</span> <span class=o>=</span> <span class=n>src</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>src_mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>src</span> <span class=o>!=</span> <span class=n>pad</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>tgt</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>tgt</span> <span class=o>=</span> <span class=n>tgt</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>tgt_y</span> <span class=o>=</span> <span class=n>tgt</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>tgt_mask</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>make_std_mask</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>tgt</span><span class=p>,</span> <span class=n>pad</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ntokens</span> <span class=o>=</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>tgt_y</span> <span class=o>!=</span> <span class=n>pad</span><span class=p>)</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@staticmethod</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>make_std_mask</span><span class=p>(</span><span class=n>tgt</span><span class=p>,</span> <span class=n>pad</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Create a mask to hide padding and future words.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>tgt_mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>tgt</span> <span class=o>!=</span> <span class=n>pad</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>tgt_mask</span> <span class=o>=</span> <span class=n>tgt_mask</span> <span class=o>&amp;</span> <span class=n>subsequent_mask</span><span class=p>(</span><span class=n>tgt</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>type_as</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>tgt_mask</span><span class=o>.</span><span class=n>data</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>tgt_mask</span>
</span></span></code></pre></div><div class=qa-block><div class=qa-content><div class=qa-question-section><div class=qa-label>$Q$</div><div class=qa-text>Why is the shape <code>(nbatches, n_seq)</code> sometimes described as <code>(nbatches, n_seq, vocab)</code> if each token ID is just a scalar value?</div></div><div class=qa-answer-section><div class=qa-label>$A$</div><div class=qa-text><p>In the original paper, the authors simply state that they use <em>learned embeddings</em> to map token IDs to vectors of dimension $d_{model}$, without mentioning one-hot explicitly.</p><p>Mathematically, however, you can think of each token as a one-hot vector, so that the embedding operation becomes</p><p>$$x_{\text{emb}} = \text{onehot}(token_{id}) \times W_{emb}.$$</p><p>This is convenient because it lets us express the embedding as a standard matrix multiplication.</p><p>For example, the sentence <code>"I love you"</code> with token IDs <code>[40, 3047, 481, 0, 0, 0]</code> would look like:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mathematica data-lang=mathematica><span class=line><span class=cl><span class=n>O</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=p>,</span><span class=w> </span><span class=mi>1</span><span class=p>(</span><span class=n>at</span><span class=w> </span><span class=mi>40</span><span class=p>),</span><span class=w> </span><span class=err>...</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>]</span><span class=w>    </span><span class=nv>#</span><span class=w> </span><span class=s>&#34;I&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>O</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>1</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=p>,</span><span class=w> </span><span class=mi>1</span><span class=p>(</span><span class=n>at</span><span class=w> </span><span class=mi>3047</span><span class=p>),</span><span class=w> </span><span class=err>...</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>]</span><span class=w>  </span><span class=nv>#</span><span class=w> </span><span class=s>&#34;love&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>O</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>2</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=p>,</span><span class=w> </span><span class=mi>1</span><span class=p>(</span><span class=n>at</span><span class=w> </span><span class=mi>481</span><span class=p>),</span><span class=w> </span><span class=err>...</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=s>&#34;you&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>O</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>3</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mi>1</span><span class=p>(</span><span class=n>at</span><span class=w> </span><span class=mi>0</span><span class=p>),</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>]</span><span class=w>          </span><span class=nv>#</span><span class=w> </span><span class=p>[</span><span class=n>PAD</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>O</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>4</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mi>1</span><span class=p>(</span><span class=n>at</span><span class=w> </span><span class=mi>0</span><span class=p>),</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>]</span><span class=w>          </span><span class=nv>#</span><span class=w> </span><span class=p>[</span><span class=n>PAD</span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>O</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>5</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mi>1</span><span class=p>(</span><span class=n>at</span><span class=w> </span><span class=mi>0</span><span class=p>),</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>]</span><span class=w>          </span><span class=nv>#</span><span class=w> </span><span class=p>[</span><span class=n>PAD</span><span class=p>]</span><span class=w>
</span></span></span></code></pre></div><p>Of course, this representation is very inefficient in practice (huge memory cost).</p><p>So in real implementations (e.g. PyTorch), we directly use the <code>(nbatches, n_seq)</code> token ID tensor to <em>index</em> into <code>W_emb</code> and fetch the corresponding rows.</p><p><strong>In practice:</strong> think of <code>vocab</code> as the <em>number of unique token IDs (vocabulary size)</em>, not as an actual one-hot dimension in the input.</p></div></div></div></div><h3 id=token-embedding>Token Embedding</h3><p><strong>input shape: <code>(nbatches, n_seq)</code></strong></p><p>Now the Transformer Architecture starts.<br>First thing we do is we embed all token in sequences within a batch.</p><p>Embedding matrix <code>W_emb</code> shape is <code>(vocab, d_model)</code>. In the original paper <code>d_model</code> is set to 512.</p><p>Input(<code>(nbatches, n_seq)</code>) will use <code>W_emb</code> as a lookup table and replace the token id into <code>d_model</code> dimension, which makes the scalar token id into <code>d_model</code> dimension vector for each token.</p><p>After embedding, we scale the elements in d_model by multiplying it with $\sqrt{d_\text{model}} = \sqrt{512}$. Original paper doesn&rsquo;t explain the reason clearly but based on this, it seems they are multiplying to keep/strengthen the token embedding information even after Positional Embedding is added. Since it&rsquo;s just scalar multiplication, this procedure doesn&rsquo;t change the shape.</p><p>output shape: <code>(nbatches, n_seq, d_model)</code></p><p>Back to our example, the sentence &ldquo;I love you&rdquo;(<code>40 ,3047, 481, 0, 0, 0</code>) will extract <code>W_emb[40]</code>, <code>W_emb[3047]</code>, <code>W_emb[481]</code>, and <code>W_emb[0]</code>.</p><table><thead><tr><th style=text-align:center>token</th><th style=text-align:center>token id</th><th style=text-align:center>d_model = 512</th></tr></thead><tbody><tr><td style=text-align:center><code>[PAD]</code></td><td style=text-align:center>0</td><td style=text-align:center><code>[ 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, ... ]</code></td></tr><tr><td style=text-align:center><code>I</code></td><td style=text-align:center>40</td><td style=text-align:center><code>[ 0.12, -0.07, 0.91, -0.04, 0.26, 0.03, -0.18, 0.55, ... ]</code></td></tr><tr><td style=text-align:center><code>love</code></td><td style=text-align:center>3047</td><td style=text-align:center><code>[ 0.33, 0.25, -0.48, 0.18, -0.09, 0.41, 0.07, -0.22, ... ]</code></td></tr><tr><td style=text-align:center><code>you</code></td><td style=text-align:center>481</td><td style=text-align:center><code>[-0.55, 0.19, 0.07, 0.92, -0.14, 0.08, 0.36, -0.02, ... ]</code></td></tr><tr><td style=text-align:center><code>am</code></td><td style=text-align:center>939</td><td style=text-align:center><code>[ 0.41, 0.05, 0.36, -0.33, 0.22, -0.17, 0.10, 0.04, ... ]</code></td></tr><tr><td style=text-align:center><code>in</code></td><td style=text-align:center>306</td><td style=text-align:center><code>[ 0.29, -0.15, -0.04, 0.22, -0.11, 0.09, -0.27, 0.30, ... ]</code></td></tr><tr><td style=text-align:center><code>with</code></td><td style=text-align:center>483</td><td style=text-align:center><code>[-0.21, 0.56, 0.02, 0.44, 0.05, -0.08, 0.12, -0.19, ... ]</code></td></tr><tr><td style=text-align:center><code>,</code></td><td style=text-align:center>11</td><td style=text-align:center><code>[ 0.77, -0.10, -0.13, -0.26, 0.15, 0.06, -0.05, 0.02, ... ]</code></td></tr><tr><td style=text-align:center><code>too</code></td><td style=text-align:center>3101</td><td style=text-align:center><code>[ 0.04, 0.88, -0.29, 0.13, -0.06, 0.21, -0.12, 0.44, ... ]</code></td></tr></tbody></table><img src=/images/shaped-transformer-example-4.png alt=Image><p>After replacing based on the W_emb lookup table, &ldquo;I love you&rdquo; would be</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mathematica data-lang=mathematica><span class=line><span class=cl><span class=n>E</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.12</span><span class=p>,</span><span class=w> </span><span class=mf>-0.07</span><span class=p>,</span><span class=w>  </span><span class=mf>0.91</span><span class=p>,</span><span class=w> </span><span class=mf>-0.04</span><span class=p>,</span><span class=w>  </span><span class=mf>0.26</span><span class=p>,</span><span class=w>  </span><span class=mf>0.03</span><span class=p>,</span><span class=w> </span><span class=mf>-0.18</span><span class=p>,</span><span class=w>  </span><span class=mf>0.55</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=s>&#34;I&#34;</span><span class=w>    </span><span class=p>(</span><span class=n>id</span><span class=o>=</span><span class=mi>40</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>E</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>1</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.33</span><span class=p>,</span><span class=w>  </span><span class=mf>0.25</span><span class=p>,</span><span class=w> </span><span class=mf>-0.48</span><span class=p>,</span><span class=w>  </span><span class=mf>0.18</span><span class=p>,</span><span class=w> </span><span class=mf>-0.09</span><span class=p>,</span><span class=w>  </span><span class=mf>0.41</span><span class=p>,</span><span class=w>  </span><span class=mf>0.07</span><span class=p>,</span><span class=w> </span><span class=mf>-0.22</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=s>&#34;love&#34;</span><span class=w> </span><span class=p>(</span><span class=n>id</span><span class=o>=</span><span class=mi>3047</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>E</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>2</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mf>-0.55</span><span class=p>,</span><span class=w>  </span><span class=mf>0.19</span><span class=p>,</span><span class=w>  </span><span class=mf>0.07</span><span class=p>,</span><span class=w>  </span><span class=mf>0.92</span><span class=p>,</span><span class=w> </span><span class=mf>-0.14</span><span class=p>,</span><span class=w>  </span><span class=mf>0.08</span><span class=p>,</span><span class=w>  </span><span class=mf>0.36</span><span class=p>,</span><span class=w> </span><span class=mf>-0.02</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=s>&#34;you&#34;</span><span class=w>  </span><span class=p>(</span><span class=n>id</span><span class=o>=</span><span class=mi>481</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>E</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>3</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=p>[</span><span class=n>PAD</span><span class=p>]</span><span class=w>  </span><span class=p>(</span><span class=n>id</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>E</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>4</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=p>[</span><span class=n>PAD</span><span class=p>]</span><span class=w>  </span><span class=p>(</span><span class=n>id</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>E</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>5</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=p>[</span><span class=n>PAD</span><span class=p>]</span><span class=w>  </span><span class=p>(</span><span class=n>id</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=w>
</span></span></span></code></pre></div><p>Of course other two sentences in the batch, &ldquo;I am in love with you&rdquo; and &ldquo;I love you, too&rdquo; also will go through the same process.</p><h3 id=positional-embedding>Positional Embedding</h3><p>We won&rsquo;t go into detail about positional embedding in this post, since itself is a post-worth concept. I recommend <a href=https://kazemnejad.com/blog/transformer_architecture_positional_encoding/>this post</a> for better understanding of Positional Embedding. Since our main focus here is to see how the shapes change, I will explain mainly on that point of view.</p><p><strong>input shape: <code>(nbatches, n_seq, d_model)</code></strong></p><p>To briefly explain, Positional Embedding is adding same(<code>d_model</code>)-sized vector to each embedded token to preserve the positional information. Unlike other sequence neural networks such as LSTM or RNN, Transformer calculates all tokens parallely. Therefore, if it doesn&rsquo;t have positional information added, it wouldn&rsquo;t know the order of the token. While using $\sin$ and $\cos$ functions are not the only way to preserve positional information, it is known to be a helpful way to do that.<br>We add the positional information of <code>d_model</code> to each embedded tokens and since they have the same shape, the shape does not change.</p><p><strong>output shape: <code>(nbatches, n_seq, d_model)</code></strong></p><p>Back to our example, &ldquo;I love you&rdquo; has three positions.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mathematica data-lang=mathematica><span class=line><span class=cl><span class=n>PE</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>1.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>1.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>1.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.00</span><span class=p>,</span><span class=w>  </span><span class=mf>1.00</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=n>pos</span><span class=o>=</span><span class=mi>0</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>PE</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.84</span><span class=p>,</span><span class=w>  </span><span class=mf>0.54</span><span class=p>,</span><span class=w>  </span><span class=mf>0.84</span><span class=p>,</span><span class=w>  </span><span class=mf>0.54</span><span class=p>,</span><span class=w>  </span><span class=mf>0.84</span><span class=p>,</span><span class=w>  </span><span class=mf>0.54</span><span class=p>,</span><span class=w>  </span><span class=mf>0.84</span><span class=p>,</span><span class=w>  </span><span class=mf>0.54</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=n>pos</span><span class=o>=</span><span class=mi>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>PE</span><span class=p>[</span><span class=mi>2</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.91</span><span class=p>,</span><span class=w> </span><span class=mf>-0.42</span><span class=p>,</span><span class=w>  </span><span class=mf>0.91</span><span class=p>,</span><span class=w> </span><span class=mf>-0.42</span><span class=p>,</span><span class=w>  </span><span class=mf>0.91</span><span class=p>,</span><span class=w> </span><span class=mf>-0.42</span><span class=p>,</span><span class=w>  </span><span class=mf>0.91</span><span class=p>,</span><span class=w> </span><span class=mf>-0.42</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=n>pos</span><span class=o>=</span><span class=mi>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>PE</span><span class=p>[</span><span class=mi>3</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.14</span><span class=p>,</span><span class=w> </span><span class=mf>-0.99</span><span class=p>,</span><span class=w> </span><span class=mf>0.14</span><span class=p>,</span><span class=w> </span><span class=mf>-0.99</span><span class=p>,</span><span class=w> </span><span class=mf>0.14</span><span class=p>,</span><span class=w> </span><span class=mf>-0.99</span><span class=p>,</span><span class=w> </span><span class=mf>0.14</span><span class=p>,</span><span class=w> </span><span class=mf>-0.99</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w> </span><span class=nv>#</span><span class=w> </span><span class=n>pos</span><span class=o>=</span><span class=mi>3</span><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>PE</span><span class=p>[</span><span class=mi>4</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mf>-0.76</span><span class=p>,</span><span class=w> </span><span class=mf>-0.65</span><span class=p>,</span><span class=w> </span><span class=mf>-0.76</span><span class=p>,</span><span class=w> </span><span class=mf>-0.65</span><span class=p>,</span><span class=w> </span><span class=mf>-0.76</span><span class=p>,</span><span class=w> </span><span class=mf>-0.65</span><span class=p>,</span><span class=w> </span><span class=mf>-0.76</span><span class=p>,</span><span class=w> </span><span class=mf>-0.65</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w> </span><span class=nv>#</span><span class=w> </span><span class=n>pos</span><span class=o>=</span><span class=mi>4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>PE</span><span class=p>[</span><span class=mi>5</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mf>-0.96</span><span class=p>,</span><span class=w> </span><span class=mf>0.28</span><span class=p>,</span><span class=w> </span><span class=mf>-0.96</span><span class=p>,</span><span class=w> </span><span class=mf>0.28</span><span class=p>,</span><span class=w> </span><span class=mf>-0.96</span><span class=p>,</span><span class=w> </span><span class=mf>0.28</span><span class=p>,</span><span class=w> </span><span class=mf>-0.96</span><span class=p>,</span><span class=w> </span><span class=mf>0.28</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w> </span><span class=nv>#</span><span class=w> </span><span class=n>pos</span><span class=o>=</span><span class=mi>5</span><span class=w>
</span></span></span></code></pre></div><p>We add the PE with E for every token:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mathematica data-lang=mathematica><span class=line><span class=cl><span class=n>E</span><span class=o>+</span><span class=n>PE</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.12</span><span class=o>+</span><span class=mf>0.00</span><span class=p>,</span><span class=w> </span><span class=mf>-0.07</span><span class=o>+</span><span class=mf>1.00</span><span class=p>,</span><span class=w>  </span><span class=mf>0.91</span><span class=o>+</span><span class=mf>0.00</span><span class=p>,</span><span class=w> </span><span class=mf>-0.04</span><span class=o>+</span><span class=mf>1.00</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>   </span><span class=nv>#</span><span class=w> </span><span class=s>&#34;I&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>           </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.12</span><span class=p>,</span><span class=w> </span><span class=mf>0.93</span><span class=p>,</span><span class=w> </span><span class=mf>0.91</span><span class=p>,</span><span class=w> </span><span class=mf>0.96</span><span class=p>,</span><span class=w> </span><span class=mf>0.26</span><span class=p>,</span><span class=w> </span><span class=mf>1.03</span><span class=p>,</span><span class=w> </span><span class=mf>-0.18</span><span class=p>,</span><span class=w> </span><span class=mf>1.55</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>E</span><span class=o>+</span><span class=n>PE</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>1</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.33</span><span class=o>+</span><span class=mf>0.84</span><span class=p>,</span><span class=w> </span><span class=mf>0.25</span><span class=o>+</span><span class=mf>0.54</span><span class=p>,</span><span class=w> </span><span class=mf>-0.48</span><span class=o>+</span><span class=mf>0.84</span><span class=p>,</span><span class=w> </span><span class=mf>0.18</span><span class=o>+</span><span class=mf>0.54</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>     </span><span class=nv>#</span><span class=w> </span><span class=s>&#34;love&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>           </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>1.17</span><span class=p>,</span><span class=w> </span><span class=mf>0.79</span><span class=p>,</span><span class=w> </span><span class=mf>0.36</span><span class=p>,</span><span class=w> </span><span class=mf>0.72</span><span class=p>,</span><span class=w> </span><span class=mf>0.75</span><span class=p>,</span><span class=w> </span><span class=mf>0.95</span><span class=p>,</span><span class=w> </span><span class=mf>0.91</span><span class=p>,</span><span class=w> </span><span class=mf>0.32</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>E</span><span class=o>+</span><span class=n>PE</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>2</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mf>-0.55</span><span class=o>+</span><span class=mf>0.91</span><span class=p>,</span><span class=w> </span><span class=mf>0.19-0.42</span><span class=p>,</span><span class=w> </span><span class=mf>0.07</span><span class=o>+</span><span class=mf>0.91</span><span class=p>,</span><span class=w> </span><span class=mf>0.92-0.42</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>     </span><span class=nv>#</span><span class=w> </span><span class=s>&#34;you&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>           </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.36</span><span class=p>,</span><span class=w> </span><span class=mf>-0.23</span><span class=p>,</span><span class=w> </span><span class=mf>0.98</span><span class=p>,</span><span class=w> </span><span class=mf>0.50</span><span class=p>,</span><span class=w> </span><span class=mf>0.77</span><span class=p>,</span><span class=w> </span><span class=mf>-0.34</span><span class=p>,</span><span class=w> </span><span class=mf>1.27</span><span class=p>,</span><span class=w> </span><span class=mf>-0.44</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>E</span><span class=o>+</span><span class=n>PE</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>3</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.00</span><span class=o>+</span><span class=mf>0.14</span><span class=p>,</span><span class=w> </span><span class=mf>0.00-0.99</span><span class=p>,</span><span class=w> </span><span class=mf>0.00</span><span class=o>+</span><span class=mf>0.14</span><span class=p>,</span><span class=w> </span><span class=mf>0.00-0.99</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>     </span><span class=nv>#</span><span class=w> </span><span class=p>[</span><span class=n>PAD</span><span class=p>]</span><span class=w> </span><span class=n>at</span><span class=w> </span><span class=n>pos</span><span class=o>=</span><span class=mi>3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>           </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.14</span><span class=p>,</span><span class=w> </span><span class=mf>-0.99</span><span class=p>,</span><span class=w> </span><span class=mf>0.14</span><span class=p>,</span><span class=w> </span><span class=mf>-0.99</span><span class=p>,</span><span class=w> </span><span class=mf>0.14</span><span class=p>,</span><span class=w> </span><span class=mf>-0.99</span><span class=p>,</span><span class=w> </span><span class=mf>0.14</span><span class=p>,</span><span class=w> </span><span class=mf>-0.99</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>E</span><span class=o>+</span><span class=n>PE</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>4</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.00-0.76</span><span class=p>,</span><span class=w> </span><span class=mf>0.00-0.65</span><span class=p>,</span><span class=w> </span><span class=mf>0.00-0.76</span><span class=p>,</span><span class=w> </span><span class=mf>0.00-0.65</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>     </span><span class=nv>#</span><span class=w> </span><span class=p>[</span><span class=n>PAD</span><span class=p>]</span><span class=w> </span><span class=n>at</span><span class=w> </span><span class=n>pos</span><span class=o>=</span><span class=mi>4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>           </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=mf>-0.76</span><span class=p>,</span><span class=w> </span><span class=mf>-0.65</span><span class=p>,</span><span class=w> </span><span class=mf>-0.76</span><span class=p>,</span><span class=w> </span><span class=mf>-0.65</span><span class=p>,</span><span class=w> </span><span class=mf>-0.76</span><span class=p>,</span><span class=w> </span><span class=mf>-0.65</span><span class=p>,</span><span class=w> </span><span class=mf>-0.76</span><span class=p>,</span><span class=w> </span><span class=mf>-0.65</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>E</span><span class=o>+</span><span class=n>PE</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=w> </span><span class=mi>5</span><span class=p>,</span><span class=w> </span><span class=err>:</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>[</span><span class=w> </span><span class=mf>0.00-0.96</span><span class=p>,</span><span class=w> </span><span class=mf>0.00</span><span class=o>+</span><span class=mf>0.28</span><span class=p>,</span><span class=w> </span><span class=mf>0.00-0.96</span><span class=p>,</span><span class=w> </span><span class=mf>0.00</span><span class=o>+</span><span class=mf>0.28</span><span class=p>,</span><span class=w> </span><span class=err>...</span><span class=w> </span><span class=p>]</span><span class=w>     </span><span class=nv>#</span><span class=w> </span><span class=p>[</span><span class=n>PAD</span><span class=p>]</span><span class=w> </span><span class=n>at</span><span class=w> </span><span class=n>pos</span><span class=o>=</span><span class=mi>5</span><span class=w>
</span></span></span></code></pre></div><pre><code>We can see that even though the padding tokens have no semantic meaning, they still receive a unique positional signal before being processed by the Transformer layers. 
</code></pre><p>As always, <strong>remember</strong> this process is done by all sequences in the batch! (which means this process is parallelly done in the sentences &ldquo;I am in love with you&rdquo; and &ldquo;I love you, too&rdquo; as well!)</p><h3 id=2-encoder>2. Encoder</h3><p>Now we move to the Encoder Layer. Code implementation of Encoder (and Decoder) Layer in code seems like a matryoshka doll (module inside a module inside a moduel&mldr;) but I will try to explain the big picture as clearly as possible.</p><p>In the blueprint, <strong>Encoder</strong> is consisted of $N$ Layers of <strong>Encoder Layer</strong>. (Note that Encoder and Encoder Layer are NOT the same! Also note that $N$ here and <code>nbatches</code> are totally different, independent concepts!) In the original paper $N=6$ which means we go over the Encoder layer 6 times in a single process.</p><p>Each Encoder Layer has two <strong>Sublayers</strong>: <strong>Self-Attention Sublayer</strong> first and then <strong>Feed Forward Network($FFN$) Sublayer</strong>. After each sublayers are passed, they are connected to a residual stream and then layer-wise normalized(LayerNorm).</p><p><strong>Sublayers</strong> are the most fundamental building blocks in Encoder-Decoder structure of Transformer Architecture. The two types of sublayers are already mentioned above.</p><p>Now that we went over the modules inside Encoder in a brief top-down approach, we will first go over the shape/dimension changes in Self-Attention Sublayer, then Feed Forward Network Sublayer and will expand further.</p><h3 id=2-1-self-attention-multi-head-attention>2-1. Self-Attention (Multi-Head Attention)</h3><p>Now this module is one of the most complicated part in Transformer so we will look in detail, step by step. The goal is to get a clear sense of how shape changes during each process. We are going to divide the Self-Attention process into 8 minor steps. Try to follow through!</p><img src=/images/shaped-attention-attention.png alt=Image><p>Recap: Each token are <code>d_model</code> sized vectors, where token embedded and positional embedded vectors are added.</p><p><strong>input shape: <code>(nbatches, n_seq, d_model)</code></strong></p><p>When the input enters an Encoder Layer, its first destination is the Self-Attention sublayer. To understand what happens here, we need to talk about <strong>Multi-Head Attention</strong>.</p><p>Though this post isn&rsquo;t a deep theoretical dive, the core idea is simple: instead of calculating attention just once, we use multiple &ldquo;attention heads.&rdquo; Think of it like having several experts look at the same sentence; each expert (or head) can focus on different relationships between words. Each head has its own set of learned weights (<code>W_Q</code>, <code>W_K</code>, <code>W_V</code>) to find these different relationships. In the original paper, they use 8 heads (<code>h=8</code>).</p><p>Mathetmatically we represent Attention calculation as</p><p>$$\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V$$</p><p>Let&rsquo;s break down how the actual tensor shapes transform step-by-step.</p><div class="note-block note-info"><div class=note-icon>ℹ️</div><div class=note-content><div class=note-title>Note</div><div class=note-body>From now, we will call the input as <code>x</code>.</div></div></div><p><strong>1. Project to $Q, K, V$</strong> <code>(nbatches, n_seq, d_model)</code> -> <code>(nbatches, n_seq, d_model)</code><br>We first need to get the $Q, K, V$ matrices. This is done by multiplying our input <code>x</code> with three learned weight matrices: <code>W_q</code>, <code>W_k</code>, and <code>W_v</code>. Each of the weight matrices have size of shape <code>(d_model, d_model)</code>.</p><p>Each $Q, K, V$ becomes shape <code>(nbatches, n_seq, d_model)</code></p><ul><li><code>Q = x @ W_q = (nbatches, n_seq, d_model)</code></li><li><code>K = x @ W_k = (nbatches, n_seq, d_model)</code></li><li><code>V = x @ W_v = (nbatches, n_seq, d_model)</code></li></ul><div class="note-block note-tip"><div class=note-icon>💡</div><div class=note-content><div class=note-title>Tip</div><div class=note-body><strong>A Note on Parameters:</strong> We aren&rsquo;t training <code>Q</code>, <code>K</code>, and <code>V</code> directly.<br>The actual parameters we train are the weights: <code>W_Q</code>, <code>W_K</code>, and <code>W_V</code>.</div></div></div><p><strong>2. Splitting into Heads</strong> <code>(nbatches, n_seq, d_model)</code> -> <code>(nbatches, n_seq, h, d_k)</code><br>After projection, we split <code>d_model</code> into <code>h</code> seperate heads. Since in the paper, d_k is defined as <code>d_k = d_model / h</code>, we can divide the last dimensions into <code>h</code> and <code>d_k</code> and view the shape as <code>(nbatches, n_seq, h, d_k)</code>.</p><p>In code, it will be implementing as follow:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>clones</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>N</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Produce N identical layers.&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>copy</span><span class=o>.</span><span class=n>deepcopy</span><span class=p>(</span><span class=n>module</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>N</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadedAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=o>...</span>
</span></span><span class=line><span class=cl>	<span class=bp>self</span><span class=o>.</span><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>	<span class=bp>self</span><span class=o>.</span><span class=n>h</span> <span class=o>=</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>	<span class=bp>self</span><span class=o>.</span><span class=n>linears</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>),</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=o>...</span>
</span></span><span class=line><span class=cl>	<span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span> <span class=o>=</span> <span class=p>[</span><span class=n>lin</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>nbatches</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>h</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=k>for</span> <span class=n>lin</span><span class=p>,</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linears</span><span class=p>,</span> <span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>))]</span>
</span></span><span class=line><span class=cl>	<span class=o>...</span>
</span></span></code></pre></div><p>Let&rsquo;s break down the last line.</p><p>For conveninece we make four Linear Layer of shape <code>(d_model, d_model)</code> for <code>W_q, W_k, W_v, W_o</code> and save it in ModuleList.</p><div class="note-block note-info"><div class=note-icon>ℹ️</div><div class=note-content><div class=note-title>Note</div><div class=note-body>To be accurate it&rsquo;s <code>(d_model, h, d_k)</code> but for sake of convenience, since <code>d_model = h * d_k</code>, we write as <code>(d_model, d_model)</code> for sake of convenience.</div></div></div><p>We <code>zip</code> it with tuple <code>(query, key, value)</code>. Since we don&rsquo;t need q,k,v for <code>W_o</code>, it is intended to zip with only three tuples.<br>Since <code>query, key, value</code> are all shape of <code>(nbatches, n_seq, d_model)</code>, we pass it through <code>lin(x)</code>. since the Linear Layer itself has the same input and output size, the output of query, key, value after the Linear Layer have the same shape <code>(nbatches, n_seq, d_model)</code>. However we transpose it to <code>(nbatches, h, n_seq, d_k)</code> to make matrix multiplication available for parallel processing of each heads.</p><p><strong>3. Transpose for Attention Calculation</strong><br>To perform the attention calculation ($QK^T$) across all heads at once, we need <code>n_seq</code> and <code>d_k</code> to be the last two dimensions. So we transpose the shape $Q$ into <code>(nbatches, h, n_seq, d_k)</code>. Since $K^T$ is transposed version of $K$, its shape is <code>(nbatches, h, d_k, n_seq)</code>.</p><p><strong>4. Calculate Attention Scores</strong><br>We calculate $QK^T$ and the output shape becomes <code>(nbatches, h , n_seq, n_seq)</code> due to matrix mulitplicatin.<br>After calculating, as you see in the mathematical definition, we scale it by $\frac{1}{\sqrt{d_k}}$ to make it <code>n_seq</code>-agnostic.<br>Mathematically this process is</p><p><strong>6. Mask Padding Tokens</strong><br>Padding tokens are just placeholders to match all the sentence to length of <code>n_seq</code> for the sake of matrix calculation. Therefore we need to exclude them on applying to $V$. Therefore we mask the padding tokens in the key. We don&rsquo;t need to mask the query matrix since it matrix multiplies with masked key tokens and automatically gets eliminated.</p><p><strong>7. Apply Softmax</strong><br>We apply a softmax function to the scores, which turns them into positive values that sum to 1. This converts the scores into attention <strong>weights</strong>. The shape remains <code>(nbatches, h, n_seq, n_seq)</code>.</p><p><strong>8. Apply Attention to V</strong> Now we multiply our attention weights by the Value matrix <code>V</code>. This enhances the representation of each token by incorporating information from the other tokens it&rsquo;s paying attention to.<br>Attention weight shape: <code>(nbatches, h, n_seq, n_seq)</code><br><code>V</code> shape: <code>(nbatches, h, n_seq, d_v)</code> (where <code>d_v = d_k</code> in the original paper)<br>Output = <code>weights @ V</code> and its shape becomes <code>(nbatches, h, n_seq, d_v)</code></p><p><strong>9. Concatenate Heads</strong><br>The parallel processing for each head is done. Now we want to merge our <code>h</code> heads back into a single tensor. We reverse the transpose from step 3.<br>so the shape changes from <code>(nbatches, h, n_seq, d_v)</code> to <code>(nbatches, n_seq, h, d_v)</code>.</p><p><strong>10. Reshape to Final Output</strong><br>Finally, we flatten the <code>h</code> and <code>d_v</code> dimensions back into the original <code>d_model</code> dimension. This is the <code>reshape()</code> or <code>view()</code> call(in code) that concludes the process.<br>Shape changes from <code>(nbatches, n_seq, h, d_v)</code> to <code>(nbatches, n_seq, h * d_v)</code>.<br>Since <code>h * d_v = d_model</code>, our final output shape is <code>(nbatches, n_seq, d_model)</code>, which is exactly what we started with! This allows us to pass it to the next layer in the network.</p><p>This output is then passed through a final linear layer, <code>W_O</code>, which also doesn&rsquo;t change the shape.</p><p>final output shape: <code>(nbatches, n_seq, d_model)</code></p><div class="note-block note-info"><div class=note-icon>ℹ️</div><div class=note-content><div class=note-title>Note</div><div class=note-body><ul><li>In practice, each attention head receives the <strong>same full input</strong> of shape <code>(nbatches, n_seq, d_model)</code>. The difference between heads comes entirely from their learned projection weights $(W_Q, W_K, W_V)$. Heads do not split the input; instead, they naturally learn to focus on different subspaces of the same representation.</li></ul><p>Below are two common parts most people are wrong about, or didn&rsquo;t think of.</p><ul><li>Does <code>d_k</code> have to be <code>d_model / h</code>?<br>No, but it&rsquo;s a very practical choice to make.<br>The main reason is to ensure the output of the attention block matches the shape as the input. This is crucial for the residual connection(<code>x = x + MHA(x)</code>).<br>When <code>d_k = d_model / h</code>, After we concatenate the heads, the final dimension is <code>h * d_k = h * (d_model / h) = d_model</code>. The output shape <code>(..., d_model)</code> perfectly matches the input shape, so the residual connection works seamlessly. When <code>d_k</code> is different from <code>d_model / h</code>, The concatenated dimension becomes <code>h * d_k</code>, which is not equal to <code>d_model</code>. This creates a shape mismatch. To fix this, the final linear layer (<code>W_O</code>) must act as a projection, mapping the shape from <code>h * d_k</code> back to <code>d_model</code>. So it&rsquo;s just a practical and rational design choice to keep the dimension consistent.</li><li>Does <code>d_k</code> have to match <code>d_v</code>?<br><strong>No</strong>, not at all.<br>While the query and key dimensions (<code>d_q</code> and <code>d_k</code>) must be identical for the dot product to work, the value dimension (<code>d_v</code>) can be any size you want.<br>The output dimension after applying attention and concatenating the heads is always <code>h * d_v</code>. The final linear layer, <code>W_O</code>, is responsible for projecting this <code>h * d_v</code> dimension back to <code>d_model</code> to ensure the residual connection works.<br>In fact, the <code>W_O</code> layer makes the choice of <code>d_v</code> very flexible. Its job is to take whatever the heads output (<code>h * d_v</code>) and reshape it into the <code>d_model</code> dimension that the rest of the network expects. Setting <code>d_k = d_v</code> is just a common simplification.</li></ul><p>To summarize both questions above, if we just make sure <code>d_k = d_q</code> (for attention calculation) and shape of <code>W_O</code> as <code>(h * d_v, d_model)</code>, <code>d_k</code> and <code>d_v</code> can be whatever integer it can be. However by matching <code>d_k</code> and <code>d_v</code> based on <code>d_model</code> makes residual connection seamless and computationally efficient.</p></div></div></div><p>Back to our example, &ldquo;I love you&rdquo; sentence</p><h3 id=2-2-residual-connection-and-normalization>2-2. Residual Connection and Normalization</h3><h3 id=2-3-feed-forward-network-ffn>2-3. Feed Forward Network ($FFN$)</h3><p>input shape: <code>(nbatches, n_seq, d_model)</code><br>We use $FFN$ to add non-linearity to learn and represent in higher dimension.</p><p>The feed forward network&rsquo;s shape is <code>(d_model, d_model)</code> for each token. Therefore the overall shape doesn&rsquo;t change.</p><h3 id=2-4-residual-connection-and-normalization>2-4. Residual Connection and Normalization</h3><h3 id=3-decoder>3. Decoder</h3><h2 id=conclusion>Conclusion</h2><p>We have went through the common notations used in transformer, and used specific example to go through &ldquo;one training phase&rdquo; of Transformer Architecture. Finally we saw how shape changes in actual code (why all those <code>reshape</code> and <code>transpose</code> are used). Hope this post helps you gain insight and intuition of the bigger picture.</p></div><nav class=post-nav><div class=nav-prev><a href=/blog/posts/limitations-of-current-rl/ class=nav-link><span class=nav-label>previous: </span><span class=nav-title>Limitations of Current RL</span></a></div></nav></article></div></main><footer class=footer><div class=footer-content><p>&copy; 2025 jlog. Built with <a href=https://gohugo.io/>Hugo</a>.</p></div></footer><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></body></html>