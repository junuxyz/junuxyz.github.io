<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>[Paper Review] UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representation - jlog</title><meta name=description content="Personal blog and thoughts"><meta name=author content="Junu"><meta property="og:url" content="https://junuxyz.github.io/blog/posts/paper-review-uniskill-imitating-human-videos-via-cross-embodiment-skill-representation/"><meta property="og:site_name" content="jlog"><meta property="og:title" content="[Paper Review] UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representation"><meta property="og:description" content="Abstract Imitating experts is challenging due to visual, physical differences between human and robot.
Previous methods used cross-embodiment datasets with shared scenes and tasks but these data are limited which makes it hard to scale.
This paper presents a new framework called UniSkill that learns embodiment-agnostic skill representation from large video dataset.
1. Introduction UniSkill uses image-editing pipeline for the neural network to focus on capturing the dynamics changes (over static content) between temproally distant video frames."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-08-26T13:04:39+09:00"><meta property="article:modified_time" content="2025-08-26T13:04:39+09:00"><meta property="article:tag" content="Paper Review"><meta property="article:tag" content="Robot Learning"><meta property="article:tag" content="Video Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="[Paper Review] UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representation"><meta name=twitter:description content="Abstract Imitating experts is challenging due to visual, physical differences between human and robot.
Previous methods used cross-embodiment datasets with shared scenes and tasks but these data are limited which makes it hard to scale.
This paper presents a new framework called UniSkill that learns embodiment-agnostic skill representation from large video dataset.
1. Introduction UniSkill uses image-editing pipeline for the neural network to focus on capturing the dynamics changes (over static content) between temproally distant video frames."><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=/css/variables.css><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/custom.css><link rel=icon type=image/x-icon href=/favicon.ico></head><body class=blog-section><header class=header><div class=header-content><div class=logo><a href=/blog/>jlog</a></div><button class=hamburger-menu aria-label="Toggle menu" aria-expanded=false>
<span></span>
<span></span>
<span></span></button><div class=header-right><nav class=nav><a href=/blog/categories/thoughts/ class=nav-item>Thoughts</a>
<a href=/blog/categories/ml/ class=nav-item>ML</a>
<a href=/ class=nav-item>About</a>
<a href=/blog/tags/ class=nav-item>Tags</a></nav></div></div></header><main class=main><div class=container><article class=post><header class=post-header><h1 class=post-title>[Paper Review] UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representation</h1><div class=post-meta><time datetime=2025-08-26T13:04:39+09:00>August 26, 2025</time><div class=post-tags><a href=/blog/tags/paper-review/ class=tag>#Paper Review</a>
<a href=/blog/tags/robot-learning/ class=tag>#Robot Learning</a>
<a href=/blog/tags/video-learning/ class=tag>#Video Learning</a></div></div></header><div class=post-content><img src=/images/unkskill.png alt=Image><h1 id=abstract>Abstract</h1><p>Imitating experts is challenging due to visual, physical differences between human and robot.</p><p>Previous methods used cross-embodiment datasets with shared scenes and tasks but these data are limited which makes it hard to scale.</p><p>This paper presents a new framework called <strong>UniSkill</strong> that learns embodiment-agnostic skill representation from large video dataset.</p><h1 id=1-introduction>1. Introduction</h1><p>UniSkill uses image-editing pipeline for the neural network to focus on capturing the dynamics changes (over static content) between temproally distant video frames.</p><p>Since the dynamic changes in the vidoes are <em>motion,</em> the model learns the motion(dynamic) patterns as skill representations.</p><p>This enables the use of embodiment-agnostic video dataset, which means it includes videos performed by humans, leading to a lot more available training data.</p><p>Experiment result:</p><ul><li>UniSkill effectively learned cross-embodiment skill representation</li><li>generalized to unseen human prompts at test time without additional guide</li><li>enhance robustness to new objects and tasks</li><li>performance improved as more data sources were added<br>in BOTH simulation <strong>and real world.</strong></li></ul><h1 id=2-related-work>2. Related Work</h1><h2 id=latent-action-models>Latent Action Models</h2><p>Derived action-relevant information through inverse or forward dynamics models.</p><table><thead><tr><th>Method</th><th>Approach</th><th>Limitation/Differentiator</th></tr></thead><tbody><tr><td>LAPO, Genie</td><td>Learn generative interactive environments with latent actions from gameplay videos.</td><td>These methods are primarily tailored to game settings with discrete actions.</td></tr><tr><td>LAPA</td><td>Extends latent action models to real-world robotic manipulation by incorporating diverse videos, including human demonstrations.</td><td>The learned latent actions are used indirectly, only to pretrain the policy as pseudo action labels.</td></tr><tr><td><strong>UniSkill</strong></td><td>Treats latent actions as explicit skill representations.</td><td>Directly trains a skill-conditioned policy on the learned representations.</td></tr></tbody></table><h2 id=explicit-action-representation>Explicit Action Representation</h2><p>Transfers action information from human videos to robots via explicit action representations, such as 2D/3D trajectories and flow fields.</p><table><thead><tr><th>Method</th><th>Approach</th><th>Limitation/Differentiator</th></tr></thead><tbody><tr><td>MimicPlay, EgoMimic, Motion Tracks</td><td>Extract 3D human hand trajectories from multi-view videos or wearable sensor inputs.</td><td>These methods often require calibrated cameras, pose tracking, or environment-specific constraints, limiting their scalability to off-the-shelf video datasets.</td></tr><tr><td>ATM, Im2Flow2Act</td><td>Predict 2D motion paths or flows from task-labeled human videos.</td><td>These methods often require calibrated cameras, pose tracking, or environment-specific constraints, limiting their scalability to off-the-shelf video datasets.</td></tr><tr><td><strong>UniSkill</strong></td><td>Avoids any task-specific trajectory extraction or pose supervision.</td><td>Learns directly from raw RGB videos, which enables the use of diverse public human and robot datasets.</td></tr></tbody></table><h2 id=cross-embodiment-skill-discovery>Cross-Embodiment Skill Discovery</h2><p><a href=https://arxiv.org/abs/2307.09955>XSkill</a> is the most similar approach as UniSkill, using cross-embodiment for skill discovery.<br>Here&rsquo;s how it differs:</p><table><thead><tr><th>Method</th><th>Approach</th><th>Limitation/Differentiator</th></tr></thead><tbody><tr><td>XSkill</td><td>Aligns skills from human and robot videos via Sinkhorn-Knopp clustering.</td><td>This clustering with shared prototypes implicitly assumes some degree of alignment between human and robot videos. Human videos must cover the target robot task and be captured in similar environments for effective skill transfer.</td></tr><tr><td><strong>UniSkill</strong></td><td>Takes a different approach by <strong>learning predictive representations through future frame forecasting.</strong></td><td>This completely removes the need for domain or task alignment, allowing the model to benefit even from entirely unrelated human videos.</td></tr></tbody></table><h1 id=3-method>3. Method</h1><h2 id=31-problem-formulation>3.1. Problem Formulation</h2><h3 id=1-cross-embodiment-video-datasets-d_u--t_nn_a_n1><strong>1. Cross-embodiment video datasets</strong> ($D_u = {T_n}^{N_a}_{n=1}$)</h3><p>Videos that show both humans and robots doing things.</p><h3 id=2-robot-demonstration-datasets-d_a--t_nn_a_n1><strong>2. Robot demonstration datasets</strong> ($D_a = {T_n}^{N_a}_{n=1}$)</h3><p>Specifically robot videos that are &ldquo;action-labeled,&rdquo; which means they have information about what actions the robot took at each step.</p><p>$D_a$ is relatively smaller than $D_u$ ($N_u &#187; N_a$).<br>This is because there are a lot more unlabled humand/robot video data than high quality, curated, and action-labled robot video data.</p><h2 id=32-universal-skill-representation-learning>3.2: Universal Skill Representation Learning</h2><h3 id=isd-inverse-skill-dynamics-model>$ISD$ (Inverse Skill Dynamics Model)</h3><p>$$ z_t = ISD(I_t, I_{t+k}) $$</p><ul><li><strong>Input:</strong> Two temporally distant frames ($I_t$ and $I_{t+k}$) from a video $V$.</li><li>Problem: relying soley on RGB pixels/frames lead to encoding of embodiment-specific(in this case, human) details, which can hinder the learning of embodiment-agnostic behavior $z_t$.</li><li>to bypass this problem, UniSkill <strong>internally</strong> uses a depth estimation module to get depth information from these frames($I_t, I_{t+k}$).</li><li>This helps the model to understand scene’s dynamics and spatial relationships. As an output, we get a universal skill representation, $z_t$.</li></ul><h3 id=fsd-forward-skill-dynamics-model>$FSD$ (Forward Skill Dynamics Model)</h3><p>$$ I_{t+k} = FSD(I_t, z_t). $$</p><ul><li>predicts the future frame $I_{t+k}$ by given $I_t$ and $z_t$ from ISD.<ul><li>So basically UniSkill is using a form of self-supervised learning to train a universal skill representation($z_t$) that effecitvely encodes the dynamic changes between two video frames.</li><li>There would be minimal (~close to no) changes in the difference of $I_t$ and $I_{t+k}$ which are only $k$ frames apart, besides the embodiment’s dynamic components.</li><li>To prevent $z_t$ from naively assign $z_t = I_{t+k}$, we enforce an information bottleneck on $z_t$.</li><li>Specific method: reforming as an image prediction task. Same architecture/method as InstructPix2Pix but instead of using language instruction, uses $z_t$ instead.</li></ul></li></ul><h2 id=33-universal-skill-conditioned-policy>3.3. Universal Skill-Conditioned Policy</h2><p>How the robot is trained to exectue the skills it learned in the previous step.</p><p>This uses the smaller, high-quality, action-labled robot dataset $D_a$.</p><p>$$\phi^* = \text{argmax}<em>\phi E(o_t, o</em>{t+h}, a_{t:t+h}) \sim D_a [\log \pi_\phi(a_{t:t+h} | o_t, z_t)].$$</p><h3 id=breaking-down-the-formula>Breaking down the formula</h3><ul><li>$ϕ^∗$: This represents the <strong>optimal set</strong> of parameters for the policy network ($π$). The goal of the training process is to find these parameters.</li><li>$\text{argmax}_ϕ$: This means we are trying to find the value of $ϕ$ that maximizes the expression that follows it. In simple terms, we are searching for the best possible policy parameters.</li><li>$E(o_t,o_{t+h},a_{t:t+h}) \sim D_a$: This part specifies the <strong>data source</strong> for training. It means that the training is done by sampling a triplet of data points:<ul><li>$o_t$: The robot&rsquo;s observation at the current time step ($t$).</li><li>$o_{t+h}$: The robot&rsquo;s observation at a future time step ($t+h$), where $h$ is the action horizon.</li><li>$a_{t:t+h}$: The sequence of ground-truth (labled) actions from time $t$ to $t+h$.</li><li>The notation $\sim D_a$ indicates that these <strong>samples are drawn from the robot demonstration dataset</strong>, which is where the action labels are available.</li></ul></li><li>$[\logπ_ϕ(a_{t:t+h}∣o_t,z_t)]$: This is the core of the behavioral cloning objective.<ul><li>$π_ϕ$: This is the policy network with parameters $ϕ$. It is the function that we are training.</li><li>$a_{t:t+h}$: The sequence of actions that the policy is trying to predict.</li><li>$∣o_t,z_t$: These are the inputs to the policy network. The policy is &ldquo;conditioned on&rdquo; the current observation ($o_t$) and the universal skill representation ($z_t$). The skill representation $z_t$ is extracted from the robot demonstration data using the pre-trained ISD model.</li><li>$\log$: The logarithm is typically used in the training objective for stability and because maximizing the log-likelihood is equivalent to maximizing the probability.</li></ul></li></ul><p>So basically the source/distribution is on the left, the policy trying to optimized is on the right.</p><h2 id=34-cross-embodiment-imitation-with-universal-skill-representation>3.4. Cross-Embodiment Imitation with Universal Skill Representation</h2></div><nav class=post-nav><div class=nav-prev><a href=/blog/posts/lab02-deploying-dialogpt-medium-with-fastapi-docker/ class=nav-link><span class=nav-label>previous: </span><span class=nav-title>Lab02: Deploying DialoGPT-Medium with FastAPI & Docker</span></a></div><div class=nav-next><a href=/blog/posts/the-engineering-era-of-ai/ class=nav-link><span class=nav-label>next: </span><span class=nav-title>The Engineering Era of AI</span></a></div></nav></article></div></main><footer class=footer><div class=footer-content><p>&copy; 2025 jlog. Built with <a href=https://gohugo.io/>Hugo</a>.</p><div class=social-icons><a href=https://github.com/junuxyz class=social-icon aria-label=GitHub target=_blank rel=noopener>GitHub
</a><a href=mailto:jpjunior211@gmail.com class=social-icon aria-label=Email target=_blank rel=noopener>Email</a></div></div></footer><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".hamburger-menu"),e=document.querySelector(".header-right");if(t&&e){t.addEventListener("click",function(){const t=this.getAttribute("aria-expanded")==="true";this.setAttribute("aria-expanded",!t),e.classList.toggle("active")});const n=e.querySelectorAll(".nav-item");n.forEach(n=>{n.addEventListener("click",function(){t.setAttribute("aria-expanded","false"),e.classList.remove("active")})}),document.addEventListener("click",function(n){!e.contains(n.target)&&!t.contains(n.target)&&(t.setAttribute("aria-expanded","false"),e.classList.remove("active"))})}document.querySelectorAll("pre code").forEach(e=>{const t=e.parentElement;if(!t.querySelector(".code-copy-btn")){const n=document.createElement("button");n.className="code-copy-btn",n.setAttribute("aria-label","Copy code to clipboard");const s=`<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <rect x="5" y="5" width="8" height="8" rx="1" stroke="currentColor" stroke-width="1.5" fill="none"/>
                        <path d="M4 3C4 2.44772 4.44772 2 5 2H9C9.55228 2 10 2.44772 10 3V4H5C4.44772 4 4 4.44772 4 5V11C4 11.5523 4.44772 12 5 12H6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round"/>
                    </svg>`,o=`<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M13.5 4.5L6 12L2.5 8.5" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"/>
                    </svg>`;n.innerHTML=s,n.addEventListener("click",()=>{navigator.clipboard.writeText(e.textContent).then(()=>{n.innerHTML=o,n.classList.add("copied"),setTimeout(()=>{n.innerHTML=s,n.classList.remove("copied")},2e3)})}),t.style.position="relative",t.appendChild(n)}if(!t.querySelector(".code-language")){let n=e.getAttribute("data-lang");if(!n){const t=(e.className||"").match(/language-([^\s]+)/);t&&t[1]&&(n=t[1])}if(n){const e=document.createElement("span");e.className="code-language",e.textContent=n.charAt(0).toUpperCase()+n.slice(1),t.appendChild(e)}}})})</script></body></html>