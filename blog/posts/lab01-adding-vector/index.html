<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Lab01: Adding Vector - jlog</title><meta name=description content="Personal blog and thoughts"><meta name=author content="Junu"><meta property="og:url" content="https://junuxyz.github.io/blog/posts/lab01-adding-vector/"><meta property="og:site_name" content="jlog"><meta property="og:title" content="Lab01: Adding Vector"><meta property="og:description" content="This is a simple experiment to just get a feel of the abstraction PyTorch provides, and all the internal complexity hidden below. We will also compare the performance of basic vector addition between PyTorch, Triton, and CUDA.
Note: This experiment was done in NVIDIA RTX 3050ti laptop GPU
Vector Addition in PyTorch import torch size = 128 * 128 a = torch.randn(size, device='cuda') b = torch.randn(size, device='cuda') output = torch.empty_like(a) output = a + b print(&#34;PyTorch output:&#34;) print(output) empty_like(a) creates the same size, dtype, and device(‘cuda’) as the input tensor a. It does not initialize the memory into something else, but use the garbage value of it so it’s a bit faster than using torch.zeros() or torch.ones().
The exact operation of vector addition is hidden in operator + in PyTorch. Vector Addition in Triton Triton is an open source library ran by OpenAI, which aims to be easier to code than CUDA (fewer knobs to control, don’t need to know as deep as CUDA) but doesn’t lose the performance. Check out more information via here."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-20T16:26:31+09:00"><meta property="article:modified_time" content="2025-07-20T16:26:31+09:00"><meta property="article:tag" content="Labs"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Triton"><meta name=twitter:card content="summary"><meta name=twitter:title content="Lab01: Adding Vector"><meta name=twitter:description content="This is a simple experiment to just get a feel of the abstraction PyTorch provides, and all the internal complexity hidden below. We will also compare the performance of basic vector addition between PyTorch, Triton, and CUDA.
Note: This experiment was done in NVIDIA RTX 3050ti laptop GPU
Vector Addition in PyTorch import torch size = 128 * 128 a = torch.randn(size, device='cuda') b = torch.randn(size, device='cuda') output = torch.empty_like(a) output = a + b print(&#34;PyTorch output:&#34;) print(output) empty_like(a) creates the same size, dtype, and device(‘cuda’) as the input tensor a. It does not initialize the memory into something else, but use the garbage value of it so it’s a bit faster than using torch.zeros() or torch.ones().
The exact operation of vector addition is hidden in operator + in PyTorch. Vector Addition in Triton Triton is an open source library ran by OpenAI, which aims to be easier to code than CUDA (fewer knobs to control, don’t need to know as deep as CUDA) but doesn’t lose the performance. Check out more information via here."><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel=stylesheet><link rel=stylesheet href=/css/variables.css><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><link rel=stylesheet href=/css/custom.css><link rel=icon type=image/x-icon href=/favicon.ico></head><body class=blog-section><header class=header><div class=header-content><div class=logo><a href=/blog/>jlog</a></div><button class=hamburger-menu aria-label="Toggle menu" aria-expanded=false>
<span></span>
<span></span>
<span></span></button><div class=header-right><nav class=nav><a href=/blog/categories/thoughts/ class=nav-item>Thoughts</a>
<a href=/blog/categories/ml/ class=nav-item>ML</a>
<a href=/ class=nav-item>About</a>
<a href=/blog/tags/ class=nav-item>Tags</a></nav></div></div></header><main class=main><div class=container><article class=post><header class=post-header><h1 class=post-title>Lab01: Adding Vector</h1><div class=post-meta><time datetime=2025-07-20T16:26:31+09:00>July 20, 2025</time><div class=post-tags><a href=/blog/tags/labs/ class=tag>#labs</a>
<a href=/blog/tags/pytorch/ class=tag>#pytorch</a>
<a href=/blog/tags/triton/ class=tag>#triton</a></div></div></header><div class=post-content><p>This is a simple experiment to just get a feel of the abstraction PyTorch provides, and all the internal complexity hidden below. We will also compare the performance of basic vector addition between PyTorch, Triton, and CUDA.</p><p><em>Note: This experiment was done in NVIDIA RTX 3050ti laptop GPU</em></p><h3 id=vector-addition-in-pytorch>Vector Addition in PyTorch</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>size</span> <span class=o>=</span> <span class=mi>128</span> <span class=o>*</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty_like</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;PyTorch output:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><ul><li><code>empty_like(a)</code> creates the same size, dtype, and device(&lsquo;cuda&rsquo;) as the input tensor <code>a</code>. It does not initialize the memory into something else, but use the garbage value of it so it&rsquo;s a bit faster than using <code>torch.zeros()</code> or <code>torch.ones()</code>.<br>The exact operation of vector addition is hidden in operator <code>+</code> in PyTorch.</li></ul><br><h3 id=vector-addition-in-triton>Vector Addition in Triton</h3><p><a href=https://github.com/triton-lang/triton>Triton</a> is an open source library ran by OpenAI, which aims to be easier to code than CUDA (fewer knobs to control, don&rsquo;t need to know as deep as CUDA) but doesn&rsquo;t lose the performance. Check out more information via <a href=https://openai.com/index/triton/>here</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>triton.language</span> <span class=k>as</span> <span class=nn>tl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@triton.jit</span> <span class=c1># tells us this code will be compiled</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>add_kernel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>x_ptr</span><span class=p>,</span> <span class=c1># Pointer to the first input vector.</span>
</span></span><span class=line><span class=cl>    <span class=n>y_ptr</span><span class=p>,</span> <span class=c1># Pointer to the second input vector.</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ptr</span><span class=p>,</span> <span class=c1># Pointer to output vector.</span>
</span></span><span class=line><span class=cl>    <span class=n>n_elements</span><span class=p>,</span> <span class=c1># Size of the vector.</span>
</span></span><span class=line><span class=cl>    <span class=n>BLOCK_SIZE</span><span class=p>:</span> <span class=n>tl</span><span class=o>.</span><span class=n>constexpr</span><span class=p>,</span> <span class=c1># Number of elements each program should process.</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>pid</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>program_id</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>block_start</span> <span class=o>=</span> <span class=n>pid</span> <span class=o>*</span> <span class=n>BLOCK_SIZE</span>
</span></span><span class=line><span class=cl>    <span class=n>offsets</span> <span class=o>=</span> <span class=n>block_start</span> <span class=o>+</span> <span class=n>tl</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>BLOCK_SIZE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>offsets</span> <span class=o>&lt;</span> <span class=n>n_elements</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># loads x and y from GPU RAM</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>x_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>tl</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>y_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># after calculation, put output back to main memory</span>
</span></span><span class=line><span class=cl>    <span class=n>tl</span><span class=o>.</span><span class=n>store</span><span class=p>(</span><span class=n>output_ptr</span> <span class=o>+</span> <span class=n>offsets</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>triton_add</span><span class=p>(</span><span class=n>a</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>b</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty_like</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>n_elements</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>MAX_BLOCK_SIZE</span> <span class=o>=</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>    <span class=n>grid</span> <span class=o>=</span> <span class=p>(</span><span class=n>triton</span><span class=o>.</span><span class=n>cdiv</span><span class=p>(</span><span class=n>n_elements</span><span class=p>,</span> <span class=n>MAX_BLOCK_SIZE</span><span class=p>),)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>add_kernel</span><span class=p>[</span><span class=n>grid</span><span class=p>](</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>output</span><span class=p>,</span> <span class=n>n_elements</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>size</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s1>&#39;cuda&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>triton_add</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Triton output&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><p>In order to understand this, we need to understand memories and parallel programming. Since this lab is just a tiny experiment, we will not go deep into all the concepts but just explain what is happening here.</p><p><code>triton_add</code></p><ul><li><code>n_elements = output.numel()</code> calculates the total work to do based on the output size which is 128 in this case.</li><li>We set <code>BLOCK_SIZE</code>, which defines the size of the data to process.</li><li><code>grid</code> calculates how much pids are needed to execute the elements according to BLOCK_SIZE. (In this case it&rsquo;s 128/128 so we only need a single program to execute vector addition) CPU then sends the instruction to GPU to proceed vector addition.<ul><li>We will talk about why the block size is set to 128 specifically, later in some other post (!TODO: I will link that part to here!)</li><li>cdiv is used to include the leftovers after dividing into BLOCK_SIZE. (eg. 130 dimentions / BLOCK_SIZE will result in 2 instead of 1)</li></ul></li></ul><p><code>add_kernel</code></p><ul><li><strong><code>@triton.jit</code></strong> is a special decorator that makes the function into machine language(or kernel) that can be run in GPU. This means unlike <code>triton_add</code> function, in <code>add_kernel</code>, we are using GPU programming. jit is short for just-in-time, which means the code compiles just in time as it runs.</li><li>As you can see in the parameter, we use pointers for inputs and output (<code>x_ptr</code>, <code>y_ptr</code>, and <code>output_ptr</code>). This is because we load data from the GPU RAM.</li><li><code>pid</code> or program ids are unique ids(eg. 0, 1, 2&mldr;) given to the GPU. Each program(kernel) checks its id.</li><li>each program uses its id(eg. 1) and calculate it with the amount of work it should proceed defined by BLOCK_SIZE. (eg. pid=1 should start from <code>1*128</code> to <code>2*128-1</code>)</li><li>Each programs parallely proceeds the process(in this example, it would be vector addition).</li><li>mask helps to check if the offsets do not exceed the actual data range.</li><li>Now, we take inputs from their pointers and <strong>load</strong> data based on the offests and mask.</li><li>The vector addition happens in the ALU in GPU.</li><li>Then we save the result to the output_ptr in GPU RAM.</li></ul><br>### Comparing Performance<p>first tried to naively check the performance with shell&rsquo;s <code>time</code> command but figured out it was an inappropriate tool to check the actual performance between two codes.</p><p><code>time</code> command measures multiple things in the environment such as Python interpreter starting time, loading libraries, CUDA context initializing (which takes a lot longer than the actual vector addition), and the GPU operation.</p><p>The more accurate way to check performance is to measure using <code>torch.cuda.Event</code></p><p>I made a simple benchmark.py to measure the difference:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>add_triton</span> <span class=kn>import</span> <span class=n>triton_add</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>benchmark_pytorch</span><span class=p>(</span><span class=n>a</span><span class=p>,</span><span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>run_benchmark</span><span class=p>(</span><span class=n>fn</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>start_event</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>Event</span><span class=p>(</span><span class=n>enable_timing</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>end_event</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>Event</span><span class=p>(</span><span class=n>enable_timing</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Warm-up for GPU</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>fn</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>start_event</span><span class=o>.</span><span class=n>record</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>fn</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>end_event</span><span class=o>.</span><span class=n>record</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>elapsed_time_ms</span> <span class=o>=</span> <span class=n>start_event</span><span class=o>.</span><span class=n>elapsed_time</span><span class=p>(</span><span class=n>end_event</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>elapsed_time_ms</span> <span class=o>/</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span>
</span></span><span class=line><span class=cl><span class=n>size</span> <span class=o>=</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pytorch_time</span> <span class=o>=</span> <span class=n>run_benchmark</span><span class=p>(</span><span class=n>benchmark_pytorch</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>triton_time</span> <span class=o>=</span> <span class=n>run_benchmark</span><span class=p>(</span><span class=n>triton_add</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Vector size: </span><span class=si>{</span><span class=n>size</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;PyTorch average time: </span><span class=si>{</span><span class=n>pytorch_time</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2> ms&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Triton average time: </span><span class=si>{</span><span class=n>triton_time</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2> ms&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><code>torch.cuda.synchronize()</code> is used to force CPU to <strong>wait</strong> until GPU operation is done. Since the default behavior between CPU and GPU are asynchronous, we use this command to check the precise amount of time of the GPU operation.</p><p>I&rsquo;ve tried with size = 128 but it was so short the noise took too much portion so I increased the size into <code>1024 * 1024</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>❯ python benchmark.py
</span></span><span class=line><span class=cl>Vector size: <span class=m>1048576</span>
</span></span><span class=line><span class=cl>PyTorch average time: 0.124672 ms
</span></span><span class=line><span class=cl>Triton average time: 0.124037 ms
</span></span></code></pre></div><p>result was almost the same.<br>We can conclude that vector addition is so simple + PyTorch optimized it well that there seems no room for optimizing vector addition better than PyTorch. PyTorch is as good.</p><p>I guess we will have to cover things computationally heavier, such as matmul.</p><p><em>Note: Source code can be found in <a href=https://github.com/junuxyz/labs/tree/main/lab_01>here</a></em></p><p><em>Note: Ran the actual tutorial code from <a href=https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py>Triton Tutorials</a> and got the plot below:</em></p><img src=/images/lab01-performance-comparance.png alt=Image></div><nav class=post-nav><div class=nav-prev><a href=/blog/posts/trying-out-uv-as-my-new-package-managing-tool/ class=nav-link><span class=nav-label>previous: </span><span class=nav-title>Trying Out uv as my new package managing tool</span></a></div><div class=nav-next><a href=/blog/posts/fluent-python-cheat-sheet-for-newbies/ class=nav-link><span class=nav-label>next: </span><span class=nav-title>Fluent Python Cheat Sheet for Newbies</span></a></div></nav></article></div></main><footer class=footer><div class=footer-content><p>&copy; 2026 jlog. Built with <a href=https://gohugo.io/>Hugo</a>.</p><div class=social-icons><a href=https://github.com/junuxyz class=social-icon aria-label=GitHub target=_blank rel=noopener>GitHub
</a><a href=mailto:jpjunior211@gmail.com class=social-icon aria-label=Email target=_blank rel=noopener>Email</a></div></div></footer><script>document.addEventListener("DOMContentLoaded",function(){const t=document.querySelector(".hamburger-menu"),e=document.querySelector(".header-right");if(t&&e){t.addEventListener("click",function(){const t=this.getAttribute("aria-expanded")==="true";this.setAttribute("aria-expanded",!t),e.classList.toggle("active")});const n=e.querySelectorAll(".nav-item");n.forEach(n=>{n.addEventListener("click",function(){t.setAttribute("aria-expanded","false"),e.classList.remove("active")})}),document.addEventListener("click",function(n){!e.contains(n.target)&&!t.contains(n.target)&&(t.setAttribute("aria-expanded","false"),e.classList.remove("active"))})}document.querySelectorAll("pre code").forEach(e=>{const t=e.parentElement;if(!t.querySelector(".code-copy-btn")){const n=document.createElement("button");n.className="code-copy-btn",n.setAttribute("aria-label","Copy code to clipboard");const s=`<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <rect x="5" y="5" width="8" height="8" rx="1" stroke="currentColor" stroke-width="1.5" fill="none"/>
                        <path d="M4 3C4 2.44772 4.44772 2 5 2H9C9.55228 2 10 2.44772 10 3V4H5C4.44772 4 4 4.44772 4 5V11C4 11.5523 4.44772 12 5 12H6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round"/>
                    </svg>`,o=`<svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M13.5 4.5L6 12L2.5 8.5" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"/>
                    </svg>`;n.innerHTML=s,n.addEventListener("click",()=>{navigator.clipboard.writeText(e.textContent).then(()=>{n.innerHTML=o,n.classList.add("copied"),setTimeout(()=>{n.innerHTML=s,n.classList.remove("copied")},2e3)})}),t.style.position="relative",t.appendChild(n)}if(!t.querySelector(".code-language")){let n=e.getAttribute("data-lang");if(!n){const t=(e.className||"").match(/language-([^\s]+)/);t&&t[1]&&(n=t[1])}if(n){const e=document.createElement("span");e.className="code-language",e.textContent=n.charAt(0).toUpperCase()+n.slice(1),t.appendChild(e)}}})})</script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></body></html>