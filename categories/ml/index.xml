<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML on junuxyz</title>
    <link>https://junuxyz.github.io/categories/ml/</link>
    <description>Recent content in ML on junuxyz</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Aug 2025 13:04:39 +0900</lastBuildDate>
    <atom:link href="https://junuxyz.github.io/categories/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Paper Review] UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representation</title>
      <link>https://junuxyz.github.io/posts/uniskill/</link>
      <pubDate>Tue, 26 Aug 2025 13:04:39 +0900</pubDate>
      <guid>https://junuxyz.github.io/posts/uniskill/</guid>
      <description>&lt;p&gt;![Image](/images/Pasted image 20250826133219.png)&lt;/p&gt;&#xA;&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;&#xA;&lt;p&gt;Imitating experts is challenging due to visual, physical differences between human and robot.&lt;/p&gt;&#xA;&lt;p&gt;Previous methods used cross-embodiment datasets with shared scenes and tasks but these data are limited which makes it hard to scale.&lt;/p&gt;&#xA;&lt;p&gt;This paper presents a new framework called &lt;strong&gt;UniSkill&lt;/strong&gt; that learns embodiment-agnostic skill representation from large video dataset.&lt;/p&gt;&#xA;&lt;br&gt;&#xA;# 1. Introduction&#xA;&lt;p&gt;UniSkill uses image-editing pipeline for the neural network to focus on capturing the dynamics changes (over static content) between temproally distant video frames.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lab02: Deploying DialoGPT-Medium with FastAPI &amp; Docker</title>
      <link>https://junuxyz.github.io/posts/lab02-deploying-dialo-gpt-with-fastapi/</link>
      <pubDate>Wed, 20 Aug 2025 15:57:15 +0900</pubDate>
      <guid>https://junuxyz.github.io/posts/lab02-deploying-dialo-gpt-with-fastapi/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In Lab02, We are going to look at how Jupyter/Colab Notebook models, codes are actually deployed as a service.&lt;/p&gt;&#xA;&lt;img src=&#34;https://junuxyz.github.io/images/simple-chat.png&#34; alt=&#34;Image&#34;&gt;&lt;p&gt;This is a simple chatbot called &lt;em&gt;simple chat&lt;/em&gt; which is containerized in Docker and run on FastAPI.&lt;/p&gt;&#xA;&lt;p&gt;We will build and deploy this chatbot using DialoGPT as the model, use FastAPI to create API endpoints, and deploy it with Docker.&lt;/p&gt;&#xA;&lt;p&gt;The goal of this lab is to experience the end-to-end of deploying and serving a LLM model from scratch, with minimal configuration.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fluent Python Cheat Sheet for Newbies</title>
      <link>https://junuxyz.github.io/posts/fluent-python-cheat-sheet/</link>
      <pubDate>Tue, 22 Jul 2025 18:50:06 +0900</pubDate>
      <guid>https://junuxyz.github.io/posts/fluent-python-cheat-sheet/</guid>
      <description>&lt;p&gt;High-level ML frameworks and libraries (e.g., PyTorch, JAX, TensorFlow, NumPy, Triton, and many more) are mostly based on Python.&lt;/p&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve known Python for a while, but I&amp;rsquo;ve never learned it to a professional degree and wouldn&amp;rsquo;t say I&amp;rsquo;m good at Python programming. So, I decided to read &lt;em&gt;Fluent Python&lt;/em&gt; (which seems to be one of the &amp;lsquo;bible&amp;rsquo; figures of Python) to cover some topics and improve my Python programming skills.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lab01: Adding Vector</title>
      <link>https://junuxyz.github.io/posts/lab01-adding-vector/</link>
      <pubDate>Sun, 20 Jul 2025 16:26:31 +0900</pubDate>
      <guid>https://junuxyz.github.io/posts/lab01-adding-vector/</guid>
      <description>&lt;p&gt;This is a simple experiment to just get a feel of the abstraction PyTorch provides, and all the internal complexity hidden below. We will also compare the performance of basic vector addition between PyTorch, Triton, and CUDA.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Note: This experiment was done in NVIDIA RTX 3050ti laptop GPU&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;vector-addition-in-pytorch&#34;&gt;Vector Addition in PyTorch&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;torch&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cuda&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;torch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;empty_like&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;PyTorch output:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;empty_like(a)&lt;/code&gt; creates the same size, dtype, and device(&amp;lsquo;cuda&amp;rsquo;) as the input tensor &lt;code&gt;a&lt;/code&gt;. It does not initialize the memory into something else, but use the garbage value of it so it&amp;rsquo;s a bit faster than using &lt;code&gt;torch.zeros()&lt;/code&gt; or &lt;code&gt;torch.ones()&lt;/code&gt;.&#xA;The exact operation of vector addition is hidden in operator &lt;code&gt;+&lt;/code&gt; in PyTorch.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;br&gt;&#xA;&lt;h3 id=&#34;vector-addition-in-triton&#34;&gt;Vector Addition in Triton&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/triton-lang/triton&#34;&gt;Triton&lt;/a&gt; is an open source library ran by OpenAI, which aims to be easier to code than CUDA (fewer knobs to control, don&amp;rsquo;t need to know as deep as CUDA) but doesn&amp;rsquo;t lose the performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A (shallow) Dive into VSCode Debugger</title>
      <link>https://junuxyz.github.io/posts/a-shallow-dive-into-vscode-debugger/</link>
      <pubDate>Wed, 16 Jul 2025 21:45:17 +0900</pubDate>
      <guid>https://junuxyz.github.io/posts/a-shallow-dive-into-vscode-debugger/</guid>
      <description>&lt;p&gt;I know debugging skills are very important and one of the &amp;ldquo;must have&amp;rdquo; skills for developers. However I did not explicitly tried to learn how to use and utilize VSCode debugger effectively. While reading &lt;a href=&#34;https://www.learncpp.com/cpp-tutorial/using-an-integrated-debugger-stepping/&#34;&gt;this&lt;/a&gt; during my entry to c++, I thought now was the right time to look into features VS Code gives, which were worth note taking. Today is just a shallow dive and hope to learn deeper when I need it.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
