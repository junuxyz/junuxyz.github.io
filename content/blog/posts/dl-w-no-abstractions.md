+++
title = "Deep Learning with No Abstractions"
date = 2025-10-01T12:07:37+09:00
draft = true
categories = ['']
tags = ['']
+++

Deep Learning, with all abstraction layers gone, is mostly Linear Algebra.
Especially when you are dealing with Deep Learning, compared to other machine learning fields, understanding tensors and shapes as concrete as possible is important. For understanding it we can think about it in multiple abstractions with pretty drawings but if we tear down the abstraction, go to the deepest part, it's all about setting hyperparameters on the architecture and calculating matrix multiply or dot product on them.

When I say Linear Algebra, I'm not necessarily saying one should take a Linear Algebra course in full (though that will be very helpful to build your intuition). But it's just to thinking/visualising "deep learning" into calculations of matrix/vector/scalar and how the shape changes. Frame deep learning as architecting the optimal shape/environment for the neural network to "learn". Understand what each dimension means. The more you practice and adopt this mental framework, the more intuitive and natural you will understand.
